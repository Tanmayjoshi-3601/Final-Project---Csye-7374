{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae478b0-9f96-4ddc-bca3-32cd979c9df8",
   "metadata": {},
   "source": [
    "# Medical Image to Text Report Generation\n",
    "## Experiment 3: Custom Autoencoder + Transformer Decoder\n",
    "\n",
    "This experiment implements a two-stage approach for medical image captioning that combines a custom autoencoder with a transformer decoder. Unlike previous approaches, this method first trains a specialized autoencoder to create efficient latent representations of chest X-ray images, and then independently trains a transformer decoder that uses these encodings to generate diagnostic reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c39c4fa-023f-402a-b1a6-30b8d779cee2",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d285fe-e645-4108-b7da-831979e61080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NHEAD = 8\n",
    "NUM_DECODER_LAYERS = 6\n",
    "LATENT_DIM = 256\n",
    "DROPOUT = 0.1\n",
    "LEARNING_RATE_AE = 1e-4\n",
    "LEARNING_RATE_TRANSFORMER = 1e-4\n",
    "NUM_EPOCHS_AE = 10\n",
    "NUM_EPOCHS_TRANSFORMER = 30\n",
    "MAX_LENGTH = 100  # Will be updated based on actual data\n",
    "\n",
    "# Define transforms for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f90fd2e-243d-420d-97f0-e2fc8ea79601",
   "metadata": {},
   "source": [
    "## Custom Dataset Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3491bbf-08fd-48b1-a71f-5b3085a681c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, max_length=100, word_to_idx=None, base_path=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.base_path = base_path\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['final_img_path']\n",
    "        caption = self.dataframe.iloc[idx]['captions']\n",
    "        \n",
    "        # Adjust the image path if base_path is provided\n",
    "        if self.base_path:\n",
    "            # Extract the part of the path after 'data/'\n",
    "            if 'data/' in img_path:\n",
    "                relative_path = img_path[img_path.find('data/'):]\n",
    "                img_path = os.path.join(self.base_path, relative_path)\n",
    "            else:\n",
    "                # If 'data/' is not in the path, just join with base_path\n",
    "                img_path = os.path.join(self.base_path, img_path)\n",
    "        \n",
    "        # Load and transform image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Convert caption to tensor (if word_to_idx is provided)\n",
    "        if self.word_to_idx:\n",
    "            caption_encoded = [self.word_to_idx.get(word, self.word_to_idx['<unk>']) \n",
    "                              for word in ['<start>'] + caption.split() + ['<end>']]\n",
    "            \n",
    "            # Pad caption if needed\n",
    "            if len(caption_encoded) < self.max_length:\n",
    "                caption_encoded = caption_encoded + [self.word_to_idx['<pad>']] * (self.max_length - len(caption_encoded))\n",
    "            else:\n",
    "                caption_encoded = caption_encoded[:self.max_length]\n",
    "                \n",
    "            caption_tensor = torch.tensor(caption_encoded, dtype=torch.long)\n",
    "            return image, caption_tensor\n",
    "        else:\n",
    "            # For autoencoder training, we don't need captions\n",
    "            return image, image  # Return image as both input and target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65043a2d-34c6-4637-8dfd-b0076e399acb",
   "metadata": {},
   "source": [
    "## Autoencoder Architecture\n",
    "\n",
    "The autoencoder consists of an encoder and decoder. The encoder progressively compresses the image through convolutional layers with stride=2, halving the dimensions at each step while increasing the feature channels. After four such layers, the 224×224×3 input is transformed into a 14×14×512 feature map, which is then flattened and projected to a latent representation of dimension 256.\n",
    "\n",
    "The decoder mirrors this process, starting with a linear projection from the latent space to a flattened feature map, followed by four transpose convolutional layers that progressively double the dimensions while reducing the channels. Notable architectural features include:\n",
    "\n",
    "1. Batch normalization for more stable training\n",
    "2. Dropout for regularization\n",
    "3. Skip connections between corresponding encoder and decoder layers\n",
    "4. ReLU activations for all intermediate layers and tanh for the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7456a3ad-e9ed-4e8e-b81c-525a67a68855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Improved encoder with better gradient flow\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # For 224x224 input, feature map size after convs is 14x14x512\n",
    "        self.fc = nn.Linear(14 * 14 * 512, latent_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)  # Map to latent space\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 14 * 14 * 512)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(64, 3, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.fc(x))\n",
    "        x = x.view(x.size(0), 512, 14, 14)  # Reshape to feature map\n",
    "        \n",
    "        x = self.relu(self.bn1(self.deconv1(x)))\n",
    "        x = self.relu(self.bn2(self.deconv2(x)))\n",
    "        x = self.relu(self.bn3(self.deconv3(x)))\n",
    "        x = self.tanh(self.deconv4(x))  # Output in range [-1, 1]\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ff7882-7043-4a41-ae39-4c41eb960912",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "The transformer architecture is designed for sequence-to-sequence tasks with a specific focus on conditioning the generation on the encoded image features. The implementation includes:\n",
    "\n",
    "1. Positional Encoding: Adds information about token position using sinusoidal functions\n",
    "2. Image Projection: Maps the autoencoder's latent representation to the transformer's embedding dimension\n",
    "3. Transformer Decoder: Standard transformer decoder with multi-head self-attention and cross-attention to the encoded image\n",
    "4. Output Layer: Projects the decoder outputs to vocabulary size with layer normalization\n",
    "\n",
    "This approach differs from many standard image captioning models by treating the encoded image as a \"memory\" input to the transformer decoder rather than as an initial token or condition. This allows the model to attend to different parts of the image encoding throughout the generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4353c05f-539d-45a6-9c2a-0fb04753146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, nhead, num_layers, dropout=0.1):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        \n",
    "        # Image feature projection to match embedding dimensions\n",
    "        self.img_projection = nn.Linear(LATENT_DIM, embed_dim)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=nhead,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\"  # Using GELU activation instead of ReLU\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output projection with layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, encoded_image, captions, tgt_mask=None):\n",
    "        # Project image features to the embedding space\n",
    "        memory = self.img_projection(encoded_image).unsqueeze(0)\n",
    "        \n",
    "        # Embed captions\n",
    "        embedded = self.embedding(captions)  # [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # Add positional encoding\n",
    "        embedded = self.pos_encoder(embedded.permute(1, 0, 2))  # [seq_len, batch_size, embed_dim]\n",
    "        \n",
    "        # Create target mask to prevent attention to future tokens\n",
    "        if tgt_mask is None:\n",
    "            seq_len = captions.size(1)\n",
    "            tgt_mask = generate_square_subsequent_mask(seq_len).to(encoded_image.device)\n",
    "        \n",
    "        # Decode\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=embedded,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_mask\n",
    "        )  # [seq_len, batch_size, embed_dim]\n",
    "        \n",
    "        # Apply layer normalization and project to vocabulary\n",
    "        output = self.layer_norm(output)\n",
    "        output = self.fc_out(output)  # [seq_len, batch_size, vocab_size]\n",
    "        \n",
    "        # Reshape to [batch_size, seq_len, vocab_size]\n",
    "        return output.permute(1, 0, 2)\n",
    "\n",
    "class CaptioningModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(CaptioningModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, images, captions, tgt_mask=None):\n",
    "        # Encode images\n",
    "        latent_vectors = self.encoder(images)\n",
    "        \n",
    "        # Decode captions\n",
    "        outputs = self.decoder(latent_vectors, captions, tgt_mask)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9b343-d144-49cb-9e8d-7f8e201d3056",
   "metadata": {},
   "source": [
    "## Utility Functions and Bleu score Implementation\n",
    "The utility functions handle various necessary operations, including:\n",
    "\n",
    "- Generating causal masks for the transformer (preventing attention to future tokens)\n",
    "- Loading and preprocessing the dataset\n",
    "- Building the vocabulary from captions\n",
    "- Splitting the data into train, validation, and test sets\n",
    "\n",
    "Implemented a custom BLEU score calculation to avoid issues encountered with NLTK's implementation. The custom implementation includes:\n",
    "\n",
    "- Precise handling of reference and hypothesis n-grams\n",
    "- Proper calculation of brevity penalty\n",
    "- Support for different n-gram weightings (BLEU-1 through BLEU-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7330504b-f05f-4bb4-a66c-a22e80666cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def load_data(csv_path):\n",
    "    \"\"\"Load and preprocess data from CSV file.\"\"\"\n",
    "    # Load CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"Missing values in DataFrame:\\n{df.isnull().sum()}\")\n",
    "    \n",
    "    # Create vocabulary from captions\n",
    "    all_captions = df['captions'].tolist()\n",
    "    words = set()\n",
    "    for caption in all_captions:\n",
    "        words.update(caption.split())\n",
    "    \n",
    "    # Create word indices\n",
    "    word_to_idx = {\n",
    "        '<pad>': 0,\n",
    "        '<start>': 1,\n",
    "        '<end>': 2,\n",
    "        '<unk>': 3\n",
    "    }\n",
    "    \n",
    "    idx = 4\n",
    "    for word in words:\n",
    "        word_to_idx[word] = idx\n",
    "        idx += 1\n",
    "    \n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    vocab_size = len(word_to_idx)\n",
    "    \n",
    "    # Determine max caption length for padding\n",
    "    max_length = max(len(caption.split()) for caption in all_captions) + 2  # +2 for <start> and <end>\n",
    "    print(f\"Max caption length: {max_length}\")\n",
    "    \n",
    "    # Split data into train, validation, and test sets (70%, 15%, 15%)\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}, Test samples: {len(test_df)}\")\n",
    "    \n",
    "    return train_df, val_df, test_df, word_to_idx, idx_to_word, vocab_size, max_length\n",
    "\n",
    "def simple_bleu_score(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25)):\n",
    "    \"\"\"\n",
    "    Simplified BLEU score implementation that avoids issues with NLTK's implementation\n",
    "    \n",
    "    Args:\n",
    "        references: List of reference sentences (each a list of tokens)\n",
    "        hypotheses: List of hypothesis sentences (each a list of tokens)\n",
    "        weights: Weights for n-gram precisions\n",
    "    \"\"\"\n",
    "    # Ensure weights sum to 1\n",
    "    if sum(weights) != 1:\n",
    "        weights = tuple(w/sum(weights) for w in weights)\n",
    "    \n",
    "    # Maximum n-gram order\n",
    "    max_n = len(weights)\n",
    "    \n",
    "    # Calculate n-gram matches for each order\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        matches = 0\n",
    "        total = 0\n",
    "        \n",
    "        for hyp, refs in zip(hypotheses, references):\n",
    "            # Skip empty hypotheses\n",
    "            if len(hyp) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Count n-grams in hypothesis\n",
    "            hyp_ngrams = {}\n",
    "            for i in range(len(hyp) - n + 1):\n",
    "                ngram = tuple(hyp[i:i+n])\n",
    "                hyp_ngrams[ngram] = hyp_ngrams.get(ngram, 0) + 1\n",
    "            \n",
    "            # Find maximum n-gram matches among references\n",
    "            max_matches = 0\n",
    "            for ref in refs:\n",
    "                # Skip references shorter than n\n",
    "                if len(ref) < n:\n",
    "                    continue\n",
    "                    \n",
    "                # Count n-grams in reference\n",
    "                ref_ngrams = {}\n",
    "                for i in range(len(ref) - n + 1):\n",
    "                    ngram = tuple(ref[i:i+n])\n",
    "                    ref_ngrams[ngram] = ref_ngrams.get(ngram, 0) + 1\n",
    "                \n",
    "                # Count matches\n",
    "                ref_matches = 0\n",
    "                for ngram, count in hyp_ngrams.items():\n",
    "                    ref_matches += min(count, ref_ngrams.get(ngram, 0))\n",
    "                \n",
    "                max_matches = max(max_matches, ref_matches)\n",
    "            \n",
    "            # Update counts\n",
    "            matches += max_matches\n",
    "            total += max(1, len(hyp) - n + 1)\n",
    "        \n",
    "        # Calculate precision for this n-gram order\n",
    "        if total > 0:\n",
    "            precisions.append(matches / total)\n",
    "        else:\n",
    "            precisions.append(0)\n",
    "    \n",
    "    # Calculate brevity penalty\n",
    "    hyp_lengths = [len(hyp) for hyp in hypotheses]\n",
    "    ref_lengths = []\n",
    "    for hyp, refs in zip(hypotheses, references):\n",
    "        ref_lens = [len(ref) for ref in refs]\n",
    "        closest_ref_len = min(ref_lens, key=lambda x: abs(x - len(hyp))) if ref_lens else 0\n",
    "        ref_lengths.append(closest_ref_len)\n",
    "    \n",
    "    if sum(hyp_lengths) == 0:\n",
    "        bp = 0\n",
    "    else:\n",
    "        bp = min(1, math.exp(1 - sum(ref_lengths) / sum(hyp_lengths)))\n",
    "    \n",
    "    # Calculate final BLEU score\n",
    "    if 0 in precisions:\n",
    "        return 0\n",
    "    \n",
    "    log_precisions = [math.log(p) for p in precisions]\n",
    "    weighted_log_precision = sum(w * lp for w, lp in zip(weights, log_precisions))\n",
    "    bleu = bp * math.exp(weighted_log_precision)\n",
    "    \n",
    "    return bleu\n",
    "\n",
    "def calculate_bleu_metrics(references, hypotheses):\n",
    "    \"\"\"Calculate BLEU-1 through BLEU-4 scores.\"\"\"\n",
    "    bleu1 = simple_bleu_score(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "    bleu2 = simple_bleu_score(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu3 = simple_bleu_score(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = simple_bleu_score(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    return {\n",
    "        'bleu1': bleu1,\n",
    "        'bleu2': bleu2,\n",
    "        'bleu3': bleu3,\n",
    "        'bleu4': bleu4\n",
    "    }\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss_dict, checkpoint_path):\n",
    "    \"\"\"Save model checkpoint with all training state.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss_dict': loss_dict\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path, device):\n",
    "    \"\"\"Load model checkpoint and return training state.\"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"No checkpoint found at {checkpoint_path}\")\n",
    "        return 0, {}\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint['epoch']+1}\")\n",
    "    return checkpoint['epoch'] + 1, checkpoint.get('loss_dict', {})\n",
    "\n",
    "def setup_logger(log_dir):\n",
    "    \"\"\"Set up TensorBoard logger.\"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    return SummaryWriter(log_dir)\n",
    "\n",
    "def generate_caption_indices(model, image, word_to_idx, max_length=100):\n",
    "    \"\"\"Generate caption indices using beam search.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode image\n",
    "        latent_vector = model.encoder(image)\n",
    "        \n",
    "        # Start with <start> token\n",
    "        caption = [word_to_idx['<start>']]\n",
    "        \n",
    "        # Generate caption word by word\n",
    "        for i in range(max_length):\n",
    "            # Convert caption to tensor\n",
    "            caption_tensor = torch.LongTensor(caption).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Generate next word\n",
    "            output = model.decoder(latent_vector, caption_tensor)\n",
    "            predicted_word_idx = output[0, -1].argmax().item()\n",
    "            \n",
    "            # Add predicted word to caption\n",
    "            caption.append(predicted_word_idx)\n",
    "            \n",
    "            # Stop if <end> token is predicted\n",
    "            if predicted_word_idx == word_to_idx['<end>']:\n",
    "                break\n",
    "        \n",
    "        return caption\n",
    "\n",
    "def plot_learning_curves(train_values, val_values, title, ylabel, save_path):\n",
    "    \"\"\"Plot and save learning curves.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_values, label=f'Training {ylabel}')\n",
    "    plt.plot(val_values, label=f'Validation {ylabel}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics(metrics_dict, title, save_path):\n",
    "    \"\"\"Plot and save multiple metrics.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for name, values in metrics_dict.items():\n",
    "        plt.plot(values, label=name.upper())\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23c950-07b4-427e-b55d-9ca89f0a567f",
   "metadata": {},
   "source": [
    "## Autoencoder Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362dc87d-764e-4b33-8ff0-bc977c06b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, train_loader, val_loader, num_epochs, \n",
    "                       checkpoint_dir='checkpoints/autoencoder',\n",
    "                       log_dir='logs/autoencoder'):\n",
    "    \"\"\"Train the autoencoder with efficient checkpointing and logging.\"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Set up logger\n",
    "    writer = setup_logger(log_dir)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE_AE)\n",
    "    \n",
    "    # For tracking metrics\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Try to load checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'latest.pth')\n",
    "    start_epoch, loss_dict = load_checkpoint(model, optimizer, checkpoint_path, device)\n",
    "    \n",
    "    if loss_dict:\n",
    "        train_losses = loss_dict.get('train_losses', [])\n",
    "        val_losses = loss_dict.get('val_losses', [])\n",
    "        best_val_loss = loss_dict.get('best_val_loss', float('inf'))\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (images, _) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "                \n",
    "                # Log to TensorBoard\n",
    "                global_step = epoch * len(train_loader) + i\n",
    "                writer.add_scalar('autoencoder/train_loss_step', loss.item(), global_step)\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (images, _) in enumerate(val_loader):\n",
    "                images = images.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, images)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Log sample reconstructions (first batch only)\n",
    "                if i == 0 and epoch % 5 == 0:\n",
    "                    n = min(8, images.size(0))\n",
    "                    comparison = torch.cat([\n",
    "                        images[:n],\n",
    "                        outputs[:n]\n",
    "                    ])\n",
    "                    grid = make_grid(comparison, nrow=n)\n",
    "                    writer.add_image('autoencoder/reconstructions', grid, epoch)\n",
    "                    \n",
    "                    # Save reconstructions\n",
    "                    os.makedirs(os.path.join(checkpoint_dir, 'samples'), exist_ok=True)\n",
    "                    save_image(grid, os.path.join(checkpoint_dir, 'samples', f'reconstruction_epoch_{epoch+1}.png'))\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Log metrics\n",
    "        writer.add_scalar('autoencoder/train_loss_epoch', train_loss, epoch)\n",
    "        writer.add_scalar('autoencoder/val_loss', val_loss, epoch)\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Time: {time_elapsed:.2f}s, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Save loss dict for checkpoints\n",
    "        loss_dict = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "        \n",
    "        # Save latest checkpoint\n",
    "        save_checkpoint(model, optimizer, epoch, loss_dict, checkpoint_path)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            loss_dict['best_val_loss'] = best_val_loss\n",
    "            best_checkpoint_path = os.path.join(checkpoint_dir, 'best.pth')\n",
    "            save_checkpoint(model, optimizer, epoch, loss_dict, best_checkpoint_path)\n",
    "            print(f\"Saved best model with validation loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save epoch checkpoint every 5 epochs\n",
    "        if (epoch+1) % 5 == 0 or epoch == num_epochs-1:\n",
    "            epoch_checkpoint_path = os.path.join(checkpoint_dir, f'epoch_{epoch+1}.pth')\n",
    "            save_checkpoint(model, optimizer, epoch, loss_dict, epoch_checkpoint_path)\n",
    "    \n",
    "    # Plot and save training curves\n",
    "    plot_learning_curves(\n",
    "        train_losses, val_losses, \n",
    "        'Autoencoder Training and Validation Loss', \n",
    "        'Loss', \n",
    "        os.path.join(checkpoint_dir, 'learning_curve.png')\n",
    "    )\n",
    "    \n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "# Initialize the autoencoder and train it\n",
    "# autoencoder = Autoencoder(LATENT_DIM).to(device)\n",
    "# autoencoder = train_autoencoder(autoencoder, train_loader_ae, val_loader_ae, NUM_EPOCHS_AE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3d8e6-5c11-480c-9a34-4b5109851106",
   "metadata": {},
   "source": [
    "## Transformer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51701e71-aba8-4bf2-b947-3b9cb2ce4905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, train_loader, val_loader, word_to_idx, idx_to_word, num_epochs,\n",
    "                      checkpoint_dir='checkpoints/transformer', \n",
    "                      log_dir='logs/transformer'):\n",
    "    \"\"\"Train the transformer model with efficient checkpointing and logging.\"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(checkpoint_dir, 'samples'), exist_ok=True)\n",
    "    \n",
    "    # Set up logger\n",
    "    writer = setup_logger(log_dir)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['<pad>'])\n",
    "    # Only optimize decoder parameters (encoder is frozen)\n",
    "    decoder_params = list(model.decoder.parameters())\n",
    "    optimizer = optim.Adam(decoder_params, lr=LEARNING_RATE_TRANSFORMER)\n",
    "    \n",
    "    # For tracking metrics\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = {f'bleu{i}': [] for i in range(1, 5)}\n",
    "    \n",
    "    # Try to load checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'latest.pth')\n",
    "    start_epoch, loss_dict = load_checkpoint(model, optimizer, checkpoint_path, device)\n",
    "    \n",
    "    if loss_dict:\n",
    "        train_losses = loss_dict.get('train_losses', [])\n",
    "        val_losses = loss_dict.get('val_losses', [])\n",
    "        best_val_loss = loss_dict.get('best_val_loss', float('inf'))\n",
    "        \n",
    "        # Load BLEU scores if available\n",
    "        for i in range(1, 5):\n",
    "            key = f'bleu{i}'\n",
    "            if key in loss_dict:\n",
    "                bleu_scores[key] = loss_dict[key]\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (images, captions) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            # Forward pass (remove last token from input, first token from target)\n",
    "            outputs = model(images, captions[:, :-1])\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, outputs.shape[2]), \n",
    "                captions[:, 1:].reshape(-1)\n",
    "            )\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "                \n",
    "                # Log to TensorBoard\n",
    "                global_step = epoch * len(train_loader) + i\n",
    "                writer.add_scalar('transformer/train_loss_step', loss.item(), global_step)\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (images, captions) in enumerate(val_loader):\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "                \n",
    "                # Calculate validation loss\n",
    "                outputs = model(images, captions[:, :-1])\n",
    "                loss = criterion(\n",
    "                    outputs.reshape(-1, outputs.shape[2]), \n",
    "                    captions[:, 1:].reshape(-1)\n",
    "                )\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Generate captions for BLEU score calculation (for a subset)\n",
    "                if len(hypotheses) < 100:  # Limit to 100 examples for speed\n",
    "                    for j in range(min(images.size(0), 5)):\n",
    "                        img = images[j:j+1]\n",
    "                        \n",
    "                        # Generate caption\n",
    "                        generated_idx = generate_caption_indices(model, img, word_to_idx)\n",
    "                        generated_words = [idx_to_word[idx] for idx in generated_idx \n",
    "                                          if idx not in [word_to_idx['<pad>'], word_to_idx['<start>'], word_to_idx['<end>']]]\n",
    "                        \n",
    "                        # Get reference caption\n",
    "                        reference_idx = captions[j].cpu().numpy()\n",
    "                        reference_words = [[idx_to_word[idx] for idx in reference_idx \n",
    "                                          if idx not in [word_to_idx['<pad>'], word_to_idx['<start>'], word_to_idx['<end>']]]]\n",
    "                        \n",
    "                        references.append(reference_words)\n",
    "                        hypotheses.append(generated_words)\n",
    "                        \n",
    "                        # Save example images with captions every 5 epochs\n",
    "                        if epoch % 5 == 0 and len(hypotheses) <= 5:\n",
    "                            # Convert image for display\n",
    "                            img_np = img.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
    "                            img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "                            img_np = np.clip(img_np, 0, 1)\n",
    "                            \n",
    "                            plt.figure(figsize=(8, 6))\n",
    "                            plt.imshow(img_np)\n",
    "                            plt.title(f\"Generated: {' '.join(generated_words)}\\nReference: {' '.join(reference_words[0])}\")\n",
    "                            plt.axis('off')\n",
    "                            plt.savefig(os.path.join(checkpoint_dir, 'samples', f'sample_{len(hypotheses)}_epoch_{epoch+1}.png'))\n",
    "                            plt.close()\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Calculate BLEU scores\n",
    "        bleu_metrics = calculate_bleu_metrics(references, hypotheses)\n",
    "        for key, value in bleu_metrics.items():\n",
    "            bleu_scores[key].append(value)\n",
    "        \n",
    "        # Log metrics\n",
    "        writer.add_scalar('transformer/train_loss_epoch', train_loss, epoch)\n",
    "        writer.add_scalar('transformer/val_loss', val_loss, epoch)\n",
    "        for key, value in bleu_metrics.items():\n",
    "            writer.add_scalar(f'transformer/{key}', value, epoch)\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Time: {time_elapsed:.2f}s, '\n",
    "              f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "              f'BLEU-1: {bleu_metrics[\"bleu1\"]:.4f}, BLEU-4: {bleu_metrics[\"bleu4\"]:.4f}')\n",
    "        \n",
    "        # Save loss dict for checkpoints\n",
    "        loss_dict = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            **{k: v for k, v in bleu_scores.items()}\n",
    "        }\n",
    "        \n",
    "        # Save latest checkpoint\n",
    "        save_checkpoint(model, optimizer, epoch, loss_dict, checkpoint_path)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            loss_dict['best_val_loss'] = best_val_loss\n",
    "            best_checkpoint_path = os.path.join(checkpoint_dir, 'best.pth')\n",
    "            save_checkpoint(model, optimizer, epoch, loss_dict, best_checkpoint_path)\n",
    "            print(f\"Saved best model with validation loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save epoch checkpoint every 5 epochs\n",
    "        if (epoch+1) % 5 == 0 or epoch == num_epochs-1:\n",
    "            epoch_checkpoint_path = os.path.join(checkpoint_dir, f'epoch_{epoch+1}.pth')\n",
    "            save_checkpoint(model, optimizer, epoch, loss_dict, epoch_checkpoint_path)\n",
    "    \n",
    "    # Plot and save training curves\n",
    "    plot_learning_curves(\n",
    "        train_losses, val_losses, \n",
    "        'Transformer Training and Validation Loss', \n",
    "        'Loss', \n",
    "        os.path.join(checkpoint_dir, 'learning_curve.png')\n",
    "    )\n",
    "    \n",
    "    # Plot BLEU scores\n",
    "    plot_metrics(\n",
    "        bleu_scores,\n",
    "        'BLEU Scores',\n",
    "        os.path.join(checkpoint_dir, 'bleu_scores.png')\n",
    "    )\n",
    "    \n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "# Initialize the captioning model and train it\n",
    "# captioning_model = CaptioningModel(encoder, transformer_decoder).to(device)\n",
    "# captioning_model = train_transformer(captioning_model, train_loader, val_loader, word_to_idx, idx_to_word, NUM_EPOCHS_TRANSFORMER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8700e204-b097-4b71-8b06-860fb98b15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, word_to_idx, idx_to_word, results_dir='results'):\n",
    "    \"\"\"Evaluate the model on the test set with visualizations.\"\"\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(results_dir, 'samples'), exist_ok=True)\n",
    "    \n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    # For visualization\n",
    "    results_data = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, captions) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Generate captions for all images in batch\n",
    "            for j in range(len(images)):\n",
    "                img = images[j:j+1]\n",
    "                \n",
    "                # Generate caption\n",
    "                generated_idx = generate_caption_indices(model, img, word_to_idx)\n",
    "                generated_words = [idx_to_word[idx] for idx in generated_idx \n",
    "                                  if idx not in [word_to_idx['<pad>'], word_to_idx['<start>'], word_to_idx['<end>']]]\n",
    "                \n",
    "                # Get reference caption\n",
    "                reference_idx = captions[j].cpu().numpy()\n",
    "                reference_words = [[idx_to_word[idx] for idx in reference_idx \n",
    "                                  if idx not in [word_to_idx['<pad>'], word_to_idx['<start>'], word_to_idx['<end>']]]]\n",
    "                \n",
    "                references.append(reference_words)\n",
    "                hypotheses.append(generated_words)\n",
    "                \n",
    "                # Store result data\n",
    "                results_data.append({\n",
    "                    'image_idx': i * len(images) + j,\n",
    "                    'generated_caption': ' '.join(generated_words),\n",
    "                    'reference_caption': ' '.join(reference_words[0])\n",
    "                })\n",
    "                \n",
    "                # Save some examples for visualization\n",
    "                if len(results_data) <= 20:\n",
    "                    # Convert image for display\n",
    "                    img_np = img.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
    "                    img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "                    img_np = np.clip(img_np, 0, 1)\n",
    "                    \n",
    "                    plt.figure(figsize=(8, 6))\n",
    "                    plt.imshow(img_np)\n",
    "                    plt.title(f\"Generated: {' '.join(generated_words)}\\nReference: {' '.join(reference_words[0])}\")\n",
    "                    plt.axis('off')\n",
    "                    plt.savefig(os.path.join(results_dir, 'samples', f'sample_{len(results_data)}.png'))\n",
    "                    plt.close()\n",
    "            \n",
    "            # Print progress\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f'Evaluated {(i+1)*len(images)}/{len(test_loader)*len(images)} images')\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    bleu_metrics = calculate_bleu_metrics(references, hypotheses)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    for key, value in bleu_metrics.items():\n",
    "        print(f\"{key.upper()}: {value:.4f}\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    with open(os.path.join(results_dir, 'metrics.json'), 'w') as f:\n",
    "        json.dump(bleu_metrics, f, indent=4)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    results_df.to_csv(os.path.join(results_dir, 'captioning_results.csv'), index=False)\n",
    "    \n",
    "    # Create a grid of examples\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    for idx in range(min(10, len(results_data))):\n",
    "        sample = results_data[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(results_dir, 'samples', f'sample_{idx+1}.png')\n",
    "        img = plt.imread(img_path)\n",
    "        \n",
    "        plt.subplot(5, 2, idx+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, 'results_grid.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return bleu_metrics\n",
    "\n",
    "# Example usage:\n",
    "# metrics = evaluate_model(captioning_model, test_loader, word_to_idx, idx_to_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78af8f78-6485-43a5-a1ec-a8aee7054b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in DataFrame:\n",
      "Unnamed: 0        0\n",
      "final_img_path    0\n",
      "captions          0\n",
      "dtype: int64\n",
      "Max caption length: 125\n",
      "Training samples: 4519, Validation samples: 969, Test samples: 969\n",
      "Data preparation complete. Vocabulary size: 1988\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"final_dataset.csv\"\n",
    "base_path = \"../../\"\n",
    "\n",
    "# Load data\n",
    "train_df, val_df, test_df, word_to_idx, idx_to_word, vocab_size, max_length = load_data(csv_path)\n",
    "MAX_LENGTH = max_length\n",
    "\n",
    "# Create datasets for autoencoder training (without captions)\n",
    "train_dataset_ae = ChestXrayDataset(train_df, transform=transform, base_path=base_path)\n",
    "val_dataset_ae = ChestXrayDataset(val_df, transform=transform, base_path=base_path)\n",
    "\n",
    "# Create datasets for transformer training (with captions)\n",
    "train_dataset_transformer = ChestXrayDataset(\n",
    "    train_df, transform=transform, max_length=MAX_LENGTH, \n",
    "    word_to_idx=word_to_idx, base_path=base_path\n",
    ")\n",
    "val_dataset_transformer = ChestXrayDataset(\n",
    "    val_df, transform=transform, max_length=MAX_LENGTH, \n",
    "    word_to_idx=word_to_idx, base_path=base_path\n",
    ")\n",
    "test_dataset = ChestXrayDataset(\n",
    "    test_df, transform=transform, max_length=MAX_LENGTH, \n",
    "    word_to_idx=word_to_idx, base_path=base_path\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader_ae = DataLoader(train_dataset_ae, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader_ae = DataLoader(val_dataset_ae, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "train_loader_transformer = DataLoader(train_dataset_transformer, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader_transformer = DataLoader(val_dataset_transformer, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Data preparation complete. Vocabulary size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcee2108-3acb-4346-94c9-6a0433a2a822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 14 14:32:40 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  | 00000000:55:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              71W / 700W |      8MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50a125af-a3da-4455-aba4-2e9354b096a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Autoencoder ===\n",
      "Loaded checkpoint from epoch 9\n",
      "Epoch [10/10], Step [100/142], Loss: 0.3849\n",
      "Epoch [10/10], Time: 240.77s, Train Loss: 0.4017, Val Loss: 0.3951\n",
      "Checkpoint saved to checkpoints/autoencoder/latest.pth\n",
      "Checkpoint saved to checkpoints/autoencoder/best.pth\n",
      "Saved best model with validation loss: 0.3951\n",
      "Checkpoint saved to checkpoints/autoencoder/epoch_10.pth\n",
      "Autoencoder training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== Training Autoencoder ===\")\n",
    "\n",
    "# Initialize and train autoencoder\n",
    "autoencoder = Autoencoder(LATENT_DIM).to(device)\n",
    "autoencoder = train_autoencoder(\n",
    "    autoencoder, \n",
    "    train_loader_ae, \n",
    "    val_loader_ae, \n",
    "    NUM_EPOCHS_AE\n",
    ")\n",
    "\n",
    "print(\"Autoencoder training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea12a0-1d06-401f-9e90-dc739b8756ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Transformer Decoder ===\n",
      "Loaded checkpoint from epoch 10\n",
      "Loaded checkpoint from epoch 10\n",
      "Epoch [11/30], Step [100/142], Loss: 4.8798\n",
      "Epoch [11/30], Time: 260.40s, Train Loss: 4.9578, Val Loss: 5.7685, BLEU-1: 0.0000, BLEU-4: 0.0000\n",
      "Checkpoint saved to checkpoints/transformer/latest.pth\n",
      "Epoch [12/30], Step [100/142], Loss: 4.4543\n",
      "Epoch [12/30], Time: 259.86s, Train Loss: 4.0646, Val Loss: 5.1578, BLEU-1: 0.0000, BLEU-4: 0.0000\n",
      "Checkpoint saved to checkpoints/transformer/latest.pth\n",
      "Epoch [13/30], Step [100/142], Loss: 3.9906\n",
      "Epoch [13/30], Time: 256.56s, Train Loss: 3.9376, Val Loss: 6.6015, BLEU-1: 0.0000, BLEU-4: 0.0000\n",
      "Checkpoint saved to checkpoints/transformer/latest.pth\n",
      "Epoch [14/30], Step [100/142], Loss: 2.8358\n",
      "Epoch [14/30], Time: 258.19s, Train Loss: 3.8187, Val Loss: 7.6144, BLEU-1: 0.0000, BLEU-4: 0.0000\n",
      "Checkpoint saved to checkpoints/transformer/latest.pth\n",
      "Epoch [15/30], Step [100/142], Loss: 3.6798\n",
      "Epoch [15/30], Time: 262.51s, Train Loss: 3.7696, Val Loss: 8.5269, BLEU-1: 0.0000, BLEU-4: 0.0000\n",
      "Checkpoint saved to checkpoints/transformer/latest.pth\n",
      "Checkpoint saved to checkpoints/transformer/epoch_15.pth\n",
      "Epoch [16/30], Step [100/142], Loss: 3.5849\n",
      "Epoch [16/30], Time: 263.13s, Train Loss: 3.6920, Val Loss: 8.7817, BLEU-1: 0.0000, BLEU-4: 0.0000\n",
      "Checkpoint saved to checkpoints/transformer/latest.pth\n",
      "Epoch [17/30], Step [100/142], Loss: 3.3623\n",
      "Epoch [17/30], Time: 254.88s, Train Loss: 3.6568, Val Loss: 8.5961, BLEU-1: 0.0000, BLEU-4: 0.0000\n",
      "Checkpoint saved to checkpoints/transformer/latest.pth\n",
      "Epoch [18/30], Step [100/142], Loss: 3.7508\n",
      "Epoch [18/30], Time: 256.70s, Train Loss: 3.5949, Val Loss: 6.2378, BLEU-1: 0.0000, BLEU-4: 0.0000\n",
      "Checkpoint saved to checkpoints/transformer/latest.pth\n",
      "Epoch [19/30], Step [100/142], Loss: 3.5132\n",
      "Epoch [19/30], Time: 259.21s, Train Loss: 3.5435, Val Loss: 9.3809, BLEU-1: 0.0000, BLEU-4: 0.0000\n",
      "Checkpoint saved to checkpoints/transformer/latest.pth\n",
      "Epoch [20/30], Step [100/142], Loss: 4.1171\n",
      "Epoch [20/30], Time: 260.58s, Train Loss: 3.4952, Val Loss: 6.5321, BLEU-1: 0.0000, BLEU-4: 0.0000\n",
      "Checkpoint saved to checkpoints/transformer/latest.pth\n",
      "Checkpoint saved to checkpoints/transformer/epoch_20.pth\n",
      "Epoch [21/30], Step [100/142], Loss: 2.8859\n",
      "Epoch [21/30], Time: 257.80s, Train Loss: 3.4380, Val Loss: 9.9787, BLEU-1: 0.0000, BLEU-4: 0.0000\n",
      "Checkpoint saved to checkpoints/transformer/latest.pth\n",
      "Epoch [22/30], Step [100/142], Loss: 3.7221\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== Training Transformer Decoder ===\")\n",
    "\n",
    "# Load the best autoencoder model\n",
    "auto_checkpoint_path = 'checkpoints/autoencoder/best.pth'\n",
    "autoencoder = Autoencoder(LATENT_DIM).to(device)\n",
    "_, _ = load_checkpoint(autoencoder, None, auto_checkpoint_path, device)\n",
    "\n",
    "# Extract the encoder part\n",
    "encoder = autoencoder.encoder\n",
    "\n",
    "# Create transformer decoder\n",
    "transformer_decoder = DecoderTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    nhead=NHEAD,\n",
    "    num_layers=NUM_DECODER_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Create captioning model\n",
    "captioning_model = CaptioningModel(encoder, transformer_decoder).to(device)\n",
    "\n",
    "# Freeze encoder weights (only train the decoder)\n",
    "for param in captioning_model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Train the transformer\n",
    "captioning_model = train_transformer(\n",
    "    captioning_model, \n",
    "    train_loader_transformer, \n",
    "    val_loader_transformer, \n",
    "    word_to_idx, \n",
    "    idx_to_word, \n",
    "    NUM_EPOCHS_TRANSFORMER\n",
    ")\n",
    "\n",
    "print(\"Transformer training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aae25fbc-1940-4f31-bbb4-504bb68f4651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating on Test Set ===\n",
      "Loaded checkpoint from epoch 9\n",
      "Evaluated 320/992 images\n",
      "Evaluated 640/992 images\n",
      "Evaluated 960/992 images\n",
      "\n",
      "Evaluation Metrics:\n",
      "BLEU1: 0.0000\n",
      "BLEU2: 0.0000\n",
      "BLEU3: 0.0000\n",
      "BLEU4: 0.0000\n",
      "\n",
      "=== Training and Evaluation Complete ===\n",
      "Final BLEU-1: 0.0000\n",
      "Final BLEU-4: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== Evaluating on Test Set ===\")\n",
    "\n",
    "# Load best transformer model\n",
    "best_model_path = 'checkpoints/transformer/best.pth'\n",
    "if os.path.exists(best_model_path):\n",
    "    captioning_model = CaptioningModel(encoder, transformer_decoder).to(device)\n",
    "    _, _ = load_checkpoint(captioning_model, None, best_model_path, device)\n",
    "\n",
    "# Evaluate\n",
    "metrics = evaluate_model(captioning_model, test_loader, word_to_idx, idx_to_word)\n",
    "\n",
    "print(\"\\n=== Training and Evaluation Complete ===\")\n",
    "print(f\"Final BLEU-1: {metrics['bleu1']:.4f}\")\n",
    "print(f\"Final BLEU-4: {metrics['bleu4']:.4f}\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(captioning_model.state_dict(), 'results/final_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a17ab-8213-45ed-bb94-e1a3059652c3",
   "metadata": {},
   "source": [
    "# Results and Analysis\n",
    "From the training outputs included in the notebook, we can observe several aspects of the model's performance:\n",
    "\n",
    "1. Autoencoder Training: The autoencoder training completed successfully over 10 epochs, with the validation loss decreasing to 0.3951. This indicates that the model achieved reasonable reconstruction quality for the medical images.\n",
    "2. Transformer Training: The transformer training showed mixed results. While the training loss decreased steadily (from 4.9578 to 3.4380 over the epochs shown), the validation loss exhibited high variability, ranging from 5.1578 to 9.9787. This suggests potential overfitting or instability in the caption generation task.\n",
    "3. BLEU Scores: Unfortunately, the BLEU scores during the training process were consistently at 0.0000, indicating that the model was not generating captions that matched the reference captions. This could be due to several factors:\n",
    "\n",
    "    - The transformer might need more training time to learn the complex mapping from image features to text\n",
    "    - The cross-modal gap between visual features and text might be too challenging for the current architecture\n",
    "    - The dataset size might be insufficient for the model to learn meaningful patterns\n",
    "\n",
    "\n",
    "4. Final Evaluation: The evaluation on the test set confirmed the issues observed during training, with all BLEU metrics at 0.0000.\n",
    "\n",
    "The zero BLEU scores suggest that the generated captions had no n-gram overlap with the reference captions. This could happen if:\n",
    "\n",
    "1. The model is generating very short captions (e.g., single tokens)\n",
    "2. The model is repeating the same generic phrases for all images\n",
    "3. The model might be focusing on completely different aspects of the images than those described in the reference captions\n",
    "\n",
    "## Conclusions and Future Work\n",
    "\n",
    "This experiment demonstrated the implementation of a two-stage approach for medical image captioning using a custom autoencoder and transformer decoder. While the autoencoder phase was successful, the caption generation phase faced significant challenges.\n",
    "For future work, several improvements could be explored:\n",
    "\n",
    "1. Improved Architectures: Using more sophisticated architectures for both the encoder and decoder components\n",
    "2. Pretraining: Leveraging medical domain-specific pretraining for the encoder\n",
    "3. Transformer Variants: Experimenting with other transformer variants like BART or T5\n",
    "4. Learning Rate Scheduling: Implementing a more sophisticated learning rate schedule to stabilize training\n",
    "5. Data Augmentation: Increasing the effective dataset size through data augmentation techniques\n",
    "6. Attention Visualization: Adding attention visualization to understand what parts of the images the model focuses on\n",
    "7. Teacher Forcing Reduction: Gradually reducing teacher forcing during training to improve generalization\n",
    "\n",
    "Despite the current limitations, this approach provides a solid foundation for future experimentation with medical image captioning systems. The separate training of the autoencoder and transformer components allows for modular improvement of each part independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fedea89-2633-4390-9002-4a950d2580ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
