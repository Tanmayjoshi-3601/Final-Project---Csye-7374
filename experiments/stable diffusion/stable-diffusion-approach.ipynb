{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408ff4da-83b0-46d4-817d-09f8162a7c88",
   "metadata": {},
   "source": [
    "# Medical Image to Text Report Generation\n",
    "## Experiment 4: Autoencoder + Transformer Decoder\n",
    "\n",
    "This notebook implements an advanced approach to medical image captioning that draws inspiration from stable diffusion techniques. The method uses a custom autoencoder trained with GAN objectives to create a compact latent representation of X-ray images, which is then used by a transformer decoder to generate diagnostic reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b38ffa5-96a7-430a-8d10-f399d94eab1c",
   "metadata": {},
   "source": [
    "## Import Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2216d95-be48-4c6a-a810-7b8020772906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import time\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572e032b-cb11-4843-a0b7-c7a22fc69f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rouge\n",
      "  Using cached rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from rouge) (1.16.0)\n",
      "Using cached rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef429b5-873b-4f7c-b24f-22d88d07c5f5",
   "metadata": {},
   "source": [
    "## Config and Hyperparameters\n",
    "I set the hyperparameters for both the autoencoder and transformer components. This approach requires two separate training phases, so I define different epoch counts for each. The latent dimension (256) represents the size of the compressed image representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0940613-b982-49d0-ba30-9dbedd4a2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NHEAD = 8\n",
    "NUM_DECODER_LAYERS = 6\n",
    "LATENT_DIM = 256\n",
    "DROPOUT = 0.1\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS_AE = 10\n",
    "NUM_EPOCHS_TRANSFORMER = 30\n",
    "MAX_LENGTH = 100\n",
    "\n",
    "# Define transforms for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac92f24e-c133-47f1-bb34-b9503f98bf6b",
   "metadata": {},
   "source": [
    "## Dataset Class\n",
    "The dataset class handles loading images and captions, with special attention to path resolution for different environments. It also takes care of tokenizing and padding captions to a consistent length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fa56fe1-3c88-45d0-8249-8749bacb98cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, max_length=100, word_to_idx=None, base_path=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.base_path = base_path\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['final_img_path']\n",
    "        caption = self.dataframe.iloc[idx]['captions']\n",
    "        \n",
    "        # Adjust the image path if base_path is provided\n",
    "        if self.base_path:\n",
    "            # Extract the part of the path after 'data/'\n",
    "            if 'data/' in img_path:\n",
    "                relative_path = img_path[img_path.find('data/'):]\n",
    "                img_path = os.path.join(self.base_path, relative_path)\n",
    "            else:\n",
    "                # If 'data/' is not in the path, just join with base_path\n",
    "                img_path = os.path.join(self.base_path, img_path)\n",
    "        \n",
    "        # Load and transform image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Convert caption to tensor\n",
    "        caption_encoded = [self.word_to_idx.get(word, self.word_to_idx['<unk>']) \n",
    "                          for word in ['<start>'] + caption.split() + ['<end>']]\n",
    "        \n",
    "        # Pad caption if needed\n",
    "        if len(caption_encoded) < self.max_length:\n",
    "            caption_encoded = caption_encoded + [self.word_to_idx['<pad>']] * (self.max_length - len(caption_encoded))\n",
    "        else:\n",
    "            caption_encoded = caption_encoded[:self.max_length]\n",
    "            \n",
    "        caption_tensor = torch.tensor(caption_encoded, dtype=torch.long)\n",
    "        \n",
    "        return image, caption_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891da4d-8739-4595-8cb9-fd94c49695ce",
   "metadata": {},
   "source": [
    "## Autoencoder Components\n",
    "The autoencoder consists of three components:\n",
    "\n",
    "1. Encoder: A series of convolutional layers that compress the image to a latent representation\n",
    "2. Decoder: Transpose convolutional layers that reconstruct the image from the latent space\n",
    "3. Discriminator: A GAN component that helps improve the quality of generated images\n",
    "\n",
    "This architecture is inspired by stable diffusion techniques but simplified for the medical imaging domain. The encoder progressively downsamples the image while increasing the feature channels, ending with a dense layer that maps to the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1a061bb-fa71-44fb-a44b-eafc7ef9bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Autoencoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Use a simpler architecture\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # For 224x224 input, feature map size will be 14x14x512\n",
    "        self.fc = nn.Linear(14 * 14 * 512, latent_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)  # Map to latent space\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 14 * 14 * 512)\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(64, 3, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), 512, 14, 14)  # Reshape to feature map\n",
    "        \n",
    "        x = self.relu(self.deconv1(x))\n",
    "        x = self.relu(self.deconv2(x))\n",
    "        x = self.relu(self.deconv3(x))\n",
    "        x = self.tanh(self.deconv4(x))  # Output in range [-1, 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "# Simple GAN Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Simple discriminator\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # For 224x224 input, feature map size after convs is 14x14x512\n",
    "        self.fc = nn.Linear(14 * 14 * 512, 1)\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        x = self.leaky_relu(self.conv2(x))\n",
    "        x = self.leaky_relu(self.conv3(x))\n",
    "        x = self.leaky_relu(self.conv4(x))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.sigmoid(self.fc(x))  # Output probability\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b76b1a-e0c2-48e4-b7d5-ee7355b67bbc",
   "metadata": {},
   "source": [
    "## Transformer Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f9095c-5372-49f8-92d6-a40a188a9892",
   "metadata": {},
   "source": [
    "1. Positional encoding is critical for the transformer, as it provides sequential information to the otherwise position-agnostic architecture. This implementation uses the standard sinusoidal encodings proposed in the original transformer paper.\n",
    "2. The transformer decoder receives the encoded image as a \"memory\" input and generates text tokens autoregressively. It uses multi-head attention mechanisms to attend to both the encoded image and previously generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aecf8d3c-21ca-46a4-b4cb-d3f89aab74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9bac53-733a-4ab2-88cb-0737724ce883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Decoder\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, nhead, num_layers, dropout=0.1):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=nhead,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, encoded_image, captions, tgt_mask=None):\n",
    "        # Convert encoded image to the right shape for transformer\n",
    "        # encoded_image shape: [batch_size, embed_dim]\n",
    "        # We need: [1, batch_size, embed_dim] for memory in transformer\n",
    "        memory = encoded_image.unsqueeze(0)\n",
    "        \n",
    "        # Embed captions\n",
    "        # captions shape: [batch_size, seq_len]\n",
    "        embedded = self.embedding(captions)  # [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # Add positional encoding\n",
    "        embedded = self.pos_encoder(embedded.permute(1, 0, 2))  # [seq_len, batch_size, embed_dim]\n",
    "        \n",
    "        # Create target mask to prevent attention to future tokens\n",
    "        if tgt_mask is None:\n",
    "            seq_len = captions.size(1)\n",
    "            tgt_mask = generate_square_subsequent_mask(seq_len).to(device)\n",
    "        \n",
    "        # Decode\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=embedded,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_mask\n",
    "        )  # [seq_len, batch_size, embed_dim]\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        output = self.fc_out(output)  # [seq_len, batch_size, vocab_size]\n",
    "        \n",
    "        # Reshape to [batch_size, seq_len, vocab_size]\n",
    "        return output.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb63846-ad6e-4a1c-9355-54707512f3ac",
   "metadata": {},
   "source": [
    "## Complete Model\n",
    "The full model combines the encoder from the autoencoder and the transformer decoder into a cohesive pipeline. The encoder creates a latent representation of the image, which is then passed to the transformer decoder to generate the textual report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b78f6a86-c156-4ce0-853f-f7cb56921dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full image captioning model\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, images, captions, tgt_mask=None):\n",
    "        # Encode images to get latent representation\n",
    "        latent_vectors = self.encoder(images)\n",
    "        \n",
    "        # Decode using transformer decoder\n",
    "        outputs = self.decoder(latent_vectors, captions, tgt_mask)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184176c0-8851-4f57-9ab4-3c8f5ff0f9f5",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "These utility functions handle data loading, vocabulary creation, and the generation of attention masks for the transformer. The **load_data** function is particularly important as it prepares the data and constructs the vocabulary mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8aad5866-20ba-4419-b060-9c29df5aa039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to generate square mask for transformer\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(csv_path):\n",
    "    # Load CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"Missing values in DataFrame:\\n{df.isnull().sum()}\")\n",
    "    \n",
    "    # Create vocabulary from captions\n",
    "    all_captions = df['captions'].tolist()\n",
    "    words = set()\n",
    "    for caption in all_captions:\n",
    "        words.update(caption.split())\n",
    "    \n",
    "    # Create word indices\n",
    "    word_to_idx = {\n",
    "        '<pad>': 0,\n",
    "        '<start>': 1,\n",
    "        '<end>': 2,\n",
    "        '<unk>': 3\n",
    "    }\n",
    "    \n",
    "    idx = 4\n",
    "    for word in words:\n",
    "        word_to_idx[word] = idx\n",
    "        idx += 1\n",
    "    \n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    vocab_size = len(word_to_idx)\n",
    "    \n",
    "    # Determine max caption length for padding\n",
    "    max_length = max(len(caption.split()) for caption in all_captions) + 2  # +2 for <start> and <end>\n",
    "    print(f\"Max caption length: {max_length}\")\n",
    "    \n",
    "    # Split data into train, validation, and test sets (70%, 15%, 15%)\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}, Test samples: {len(test_df)}\")\n",
    "    \n",
    "    return train_df, val_df, test_df, word_to_idx, idx_to_word, vocab_size, max_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea988d38-b597-4d3b-b644-95db2fbf0906",
   "metadata": {},
   "source": [
    "## Training Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1afa17-40c7-4526-b8e4-708323055c6c",
   "metadata": {},
   "source": [
    "### Autoencoder Training\n",
    "This function implements a GAN-based training approach for the autoencoder. It alternates between:\n",
    "\n",
    "1. Training the encoder-decoder pair to minimize reconstruction error and fool the discriminator\n",
    "2. Training the discriminator to distinguish between real and reconstructed images\n",
    "\n",
    "The GAN objective helps the autoencoder generate more realistic and detailed reconstructions, which can lead to better latent representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4778e26d-6837-447a-8d4b-c4634854fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the autoencoder with GAN\n",
    "def train_autoencoder_gan(encoder, decoder, discriminator, train_loader, val_loader, device, num_epochs=20, checkpoint_dir='checkpoints/autoencoder'):\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Optimizers\n",
    "    optimizer_G = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Loss functions\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    \n",
    "    # Training metrics\n",
    "    train_recon_losses = []\n",
    "    train_gen_losses = []\n",
    "    train_disc_losses = []\n",
    "    val_recon_losses = []\n",
    "    \n",
    "    # Best validation loss for model saving\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        total_recon_loss = 0\n",
    "        total_gen_loss = 0\n",
    "        total_disc_loss = 0\n",
    "        \n",
    "        for batch_idx, (real_images, _) in enumerate(train_loader):\n",
    "            real_images = real_images.to(device)\n",
    "            batch_size = real_images.size(0)\n",
    "            \n",
    "            # Ground truth labels\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            \n",
    "            # -----------------\n",
    "            # Train Generator (Encoder-Decoder)\n",
    "            # -----------------\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Generate latent representation and reconstruct\n",
    "            latent_vectors = encoder(real_images)\n",
    "            reconstructed_images = decoder(latent_vectors)\n",
    "            \n",
    "            # Calculate reconstruction loss\n",
    "            recon_loss = reconstruction_loss(reconstructed_images, real_images)\n",
    "            \n",
    "            # Calculate generator adversarial loss\n",
    "            validity = discriminator(reconstructed_images)\n",
    "            gen_loss = adversarial_loss(validity, real_labels)\n",
    "            \n",
    "            # Combined loss\n",
    "            g_loss = recon_loss + 0.001 * gen_loss\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # -----------------\n",
    "            # Train Discriminator\n",
    "            # -----------------\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # Loss for real images\n",
    "            validity_real = discriminator(real_images)\n",
    "            d_real_loss = adversarial_loss(validity_real, real_labels)\n",
    "            \n",
    "            # Loss for fake images\n",
    "            validity_fake = discriminator(reconstructed_images.detach())\n",
    "            d_fake_loss = adversarial_loss(validity_fake, fake_labels)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            d_loss = 0.5 * (d_real_loss + d_fake_loss)\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # Save statistics\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_gen_loss += gen_loss.item()\n",
    "            total_disc_loss += d_loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], \"\n",
    "                      f\"Recon Loss: {recon_loss.item():.4f}, Gen Loss: {gen_loss.item():.4f}, Disc Loss: {d_loss.item():.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        val_recon_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_images, _ in val_loader:\n",
    "                val_images = val_images.to(device)\n",
    "                \n",
    "                # Encode and reconstruct\n",
    "                val_latent = encoder(val_images)\n",
    "                val_reconstructed = decoder(val_latent)\n",
    "                \n",
    "                # Calculate reconstruction loss\n",
    "                val_loss = reconstruction_loss(val_reconstructed, val_images)\n",
    "                val_recon_loss += val_loss.item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_train_recon = total_recon_loss / len(train_loader)\n",
    "        avg_train_gen = total_gen_loss / len(train_loader)\n",
    "        avg_train_disc = total_disc_loss / len(train_loader)\n",
    "        avg_val_recon = val_recon_loss / len(val_loader)\n",
    "        \n",
    "        # Save metrics\n",
    "        train_recon_losses.append(avg_train_recon)\n",
    "        train_gen_losses.append(avg_train_gen)\n",
    "        train_disc_losses.append(avg_train_disc)\n",
    "        val_recon_losses.append(avg_val_recon)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        time_elapsed = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Time: {time_elapsed:.2f}s, \"\n",
    "              f\"Train Recon Loss: {avg_train_recon:.4f}, Train Gen Loss: {avg_train_gen:.4f}, \"\n",
    "              f\"Train Disc Loss: {avg_train_disc:.4f}, Val Recon Loss: {avg_val_recon:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_recon < best_val_loss:\n",
    "            best_val_loss = avg_val_recon\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                'train_recon_loss': avg_train_recon,\n",
    "                'val_recon_loss': avg_val_recon,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Saved best model with validation loss: {avg_val_recon:.4f}\")\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                'train_recon_loss': avg_train_recon,\n",
    "                'val_recon_loss': avg_val_recon,\n",
    "            }, checkpoint_path)\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_recon_losses, label='Train Reconstruction Loss')\n",
    "    plt.plot(val_recon_losses, label='Validation Reconstruction Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Reconstruction Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_gen_losses, label='Generator Loss')\n",
    "    plt.plot(train_disc_losses, label='Discriminator Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Adversarial Losses')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(checkpoint_dir, 'autoencoder_training_curves.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        'train_recon_losses': train_recon_losses,\n",
    "        'train_gen_losses': train_gen_losses,\n",
    "        'train_disc_losses': train_disc_losses,\n",
    "        'val_recon_losses': val_recon_losses\n",
    "    }\n",
    "    np.save(os.path.join(checkpoint_dir, 'training_metrics.npy'), metrics)\n",
    "    \n",
    "    return encoder, decoder, discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653b552-2dca-4ccc-a234-0f331db05ac4",
   "metadata": {},
   "source": [
    "## Transformer Training\n",
    "After the autoencoder is trained, this function handles training the transformer decoder component. It:\n",
    "\n",
    "1. Processes batches of images and captions\n",
    "2. Calculates loss using cross-entropy (ignoring padding tokens)\n",
    "3. Performs backpropagation and optimization\n",
    "4. Evaluates on the validation set using BLEU scores\n",
    "5. Saves checkpoints and training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b1ba930-1e1d-4b49-9c2d-2fb090c61e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the transformer model\n",
    "def train_transformer(model, train_loader, val_loader, word_to_idx, idx_to_word, criterion, optimizer, num_epochs=30, checkpoint_dir='checkpoints/transformer'):\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Training metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = []\n",
    "    \n",
    "    # Best validation loss for model saving\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (images, captions) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass (remove last token from input, first token from target)\n",
    "            outputs = model(images, captions[:, :-1])\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, outputs.shape[2]), \n",
    "                captions[:, 1:].reshape(-1)\n",
    "            )\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_refs = []\n",
    "        all_hyps = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_images, val_captions in val_loader:\n",
    "                val_images = val_images.to(device)\n",
    "                val_captions = val_captions.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(val_images, val_captions[:, :-1])\n",
    "                \n",
    "                # Calculate loss\n",
    "                batch_loss = criterion(\n",
    "                    outputs.reshape(-1, outputs.shape[2]), \n",
    "                    val_captions[:, 1:].reshape(-1)\n",
    "                )\n",
    "                val_loss += batch_loss.item()\n",
    "                \n",
    "                # Generate captions for BLEU score\n",
    "                for i in range(min(5, val_images.size(0))):  # Only evaluate first 5 images per batch to save time\n",
    "                    img = val_images[i].unsqueeze(0)\n",
    "                    \n",
    "                    # Generate caption\n",
    "                    generated_idx = generate_caption_indices(model, img, word_to_idx)\n",
    "                    \n",
    "                    # Convert to words\n",
    "                    generated_caption = [idx_to_word[idx] for idx in generated_idx \n",
    "                                       if idx not in [word_to_idx['<pad>'], word_to_idx['<start>'], word_to_idx['<end>']]]\n",
    "                    \n",
    "                    # Get reference caption\n",
    "                    reference_idx = val_captions[i].cpu().numpy()\n",
    "                    reference_caption = [[idx_to_word[idx] for idx in reference_idx \n",
    "                                        if idx not in [word_to_idx['<pad>'], word_to_idx['<start>'], word_to_idx['<end>']]]]\n",
    "                    \n",
    "                    all_refs.append(reference_caption)\n",
    "                    all_hyps.append(generated_caption)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        bleu1 = corpus_bleu(all_refs, all_hyps, weights=(1, 0, 0, 0))\n",
    "        bleu4 = corpus_bleu(all_refs, all_hyps, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        # Save metrics\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        bleu_scores.append((bleu1, bleu4))\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        time_elapsed = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Time: {time_elapsed:.2f}s, \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"BLEU-1: {bleu1:.4f}, BLEU-4: {bleu4:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'bleu1': bleu1,\n",
    "                'bleu4': bleu4,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Saved best model with validation loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'bleu1': bleu1,\n",
    "                'bleu4': bleu4,\n",
    "            }, checkpoint_path)\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    bleu1_scores = [b[0] for b in bleu_scores]\n",
    "    bleu4_scores = [b[1] for b in bleu_scores]\n",
    "    plt.plot(bleu1_scores, label='BLEU-1')\n",
    "    plt.plot(bleu4_scores, label='BLEU-4')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.title('BLEU Scores')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(checkpoint_dir, 'transformer_training_curves.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'bleu1_scores': bleu1_scores,\n",
    "        'bleu4_scores': bleu4_scores\n",
    "    }\n",
    "    np.save(os.path.join(checkpoint_dir, 'training_metrics.npy'), metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ddda4-5fbb-47ab-a72c-58b71cbf0699",
   "metadata": {},
   "source": [
    "## Generation and Evaluation Functions\n",
    "These functions handle inference with the trained model. **generate_caption_indices** performs autoregressive generation, predicting one token at a time until an end token is produced. **generate_caption** converts these indices to human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5502aca9-5c14-4e22-90af-5db0152dca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate caption indices for an image\n",
    "def generate_caption_indices(model, image, word_to_idx, max_length=100):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode image\n",
    "        latent_vector = model.encoder(image)\n",
    "        \n",
    "        # Initialize caption with START token\n",
    "        caption = [word_to_idx['<start>']]\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            # Convert current caption to tensor\n",
    "            caption_tensor = torch.LongTensor(caption).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Generate next word prediction\n",
    "            output = model(image, caption_tensor)\n",
    "            \n",
    "            # Get the predicted next word\n",
    "            predicted_word_idx = output[0, -1].argmax().item()\n",
    "            \n",
    "            # Add predicted word to caption\n",
    "            caption.append(predicted_word_idx)\n",
    "            \n",
    "            # Stop if END token is predicted\n",
    "            if predicted_word_idx == word_to_idx['<end>']:\n",
    "                break\n",
    "        \n",
    "        return caption\n",
    "\n",
    "# Function to generate a readable caption for an image\n",
    "def generate_caption(model, image, word_to_idx, idx_to_word, max_length=100):\n",
    "    # Get caption indices\n",
    "    caption_indices = generate_caption_indices(model, image, word_to_idx, max_length)\n",
    "    \n",
    "    # Convert indices to words, removing special tokens\n",
    "    caption_words = [idx_to_word[idx] for idx in caption_indices \n",
    "                   if idx not in [word_to_idx['<pad>'], word_to_idx['<start>'], word_to_idx['<end>']]]\n",
    "    \n",
    "    return ' '.join(caption_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406eeb32-7c64-4154-9ceb-b1730cc8e386",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "The evaluation function is comprehensive, calculating:\n",
    "\n",
    "1. BLEU scores at multiple n-gram levels\n",
    "2. METEOR scores for semantic similarity\n",
    "3. ROUGE scores for recall-oriented metrics\n",
    "4. Visual examples with both generated and reference captions\n",
    "\n",
    "This allows for a thorough assessment of model performance beyond just BLEU scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd4c8d36-5580-4e12-aef0-6968ab6a54cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model on test set\n",
    "def evaluate_model(model, test_loader, word_to_idx, idx_to_word, results_dir='results'):\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    all_refs = []\n",
    "    all_hyps = []\n",
    "    all_image_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, captions) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            for j in range(images.size(0)):\n",
    "                img = images[j].unsqueeze(0)\n",
    "                \n",
    "                # Generate caption\n",
    "                generated_idx = generate_caption_indices(model, img, word_to_idx)\n",
    "                generated_caption = [idx_to_word[idx] for idx in generated_idx \n",
    "                                   if idx not in [word_to_idx['<pad>'], word_to_idx['<start>'], word_to_idx['<end>']]]\n",
    "                \n",
    "                # Get reference caption\n",
    "                reference_idx = captions[j].cpu().numpy()\n",
    "                reference_caption = [[idx_to_word[idx] for idx in reference_idx \n",
    "                                    if idx not in [word_to_idx['<pad>'], word_to_idx['<start>'], word_to_idx['<end>']]]]\n",
    "                \n",
    "                all_refs.append(reference_caption)\n",
    "                all_hyps.append(generated_caption)\n",
    "                all_image_ids.append(f\"img_{i}_{j}\")\n",
    "                \n",
    "                # Save some examples (first 10 images)\n",
    "                if len(all_image_ids) <= 10:\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    # Denormalize image\n",
    "                    img_np = img.cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "                    img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "                    img_np = np.clip(img_np, 0, 1)\n",
    "                    \n",
    "                    plt.imshow(img_np)\n",
    "                    plt.title(f\"Generated: {' '.join(generated_caption)}\\nReference: {' '.join(reference_caption[0])}\")\n",
    "                    plt.axis('off')\n",
    "                    plt.savefig(os.path.join(results_dir, f'example_{len(all_image_ids)}.png'))\n",
    "                    plt.close()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    bleu1 = corpus_bleu(all_refs, all_hyps, weights=(1, 0, 0, 0))\n",
    "    bleu2 = corpus_bleu(all_refs, all_hyps, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu3 = corpus_bleu(all_refs, all_hyps, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(all_refs, all_hyps, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    # Calculate METEOR score\n",
    "    meteor_scores = []\n",
    "    for i in range(len(all_hyps)):\n",
    "        meteor_scores.append(meteor_score(all_refs[i], all_hyps[i]))\n",
    "    avg_meteor = np.mean(meteor_scores)\n",
    "    \n",
    "    # Calculate ROUGE score\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores([' '.join(h) for h in all_hyps], [' '.join(r[0]) for r in all_refs], avg=True)\n",
    "    \n",
    "    # Print and save metrics\n",
    "    print(f\"BLEU-1: {bleu1:.4f}\")\n",
    "    print(f\"BLEU-2: {bleu2:.4f}\")\n",
    "    print(f\"BLEU-3: {bleu3:.4f}\")\n",
    "    print(f\"BLEU-4: {bleu4:.4f}\")\n",
    "    print(f\"METEOR: {avg_meteor:.4f}\")\n",
    "    print(f\"ROUGE-1 F1: {rouge_scores['rouge-1']['f']:.4f}\")\n",
    "    print(f\"ROUGE-2 F1: {rouge_scores['rouge-2']['f']:.4f}\")\n",
    "    print(f\"ROUGE-L F1: {rouge_scores['rouge-l']['f']:.4f}\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    metrics = {\n",
    "        'bleu1': bleu1,\n",
    "        'bleu2': bleu2,\n",
    "        'bleu3': bleu3,\n",
    "        'bleu4': bleu4,\n",
    "        'meteor': avg_meteor,\n",
    "        'rouge1_f1': rouge_scores['rouge-1']['f'],\n",
    "        'rouge2_f1': rouge_scores['rouge-2']['f'],\n",
    "        'rougeL_f1': rouge_scores['rouge-l']['f']\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(results_dir, 'metrics.txt'), 'w') as f:\n",
    "        for k, v in metrics.items():\n",
    "            f.write(f\"{k}: {v:.4f}\\n\")\n",
    "    \n",
    "    # Save generated captions\n",
    "    captions_df = pd.DataFrame({\n",
    "        'image_id': all_image_ids,\n",
    "        'reference_caption': [' '.join(r[0]) for r in all_refs],\n",
    "        'generated_caption': [' '.join(h) for h in all_hyps]\n",
    "    })\n",
    "    captions_df.to_csv(os.path.join(results_dir, 'generated_captions.csv'), index=False)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50211a37-92f4-4d22-bffc-420c42e4ea7a",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "The main function orchestrates the entire training pipeline:\n",
    "\n",
    "1. Loads and preprocesses the data\n",
    "2. Initializes all model components\n",
    "3. Trains the autoencoder with GAN objectives\n",
    "4. Uses the best encoder from the autoencoder training (freezing its weights)\n",
    "5. Trains the transformer decoder with the encoded image features\n",
    "6. Evaluates the final model on the test set\n",
    "\n",
    "This two-stage training approach allows each component to be optimized for its specific task before being combined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cbd6341-e6e7-49af-954d-d2150acb44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the entire pipeline\n",
    "def main(csv_path, resume_training=False, resume_from=None):\n",
    "    # Set up checkpoint and results directories\n",
    "    checkpoint_dir_ae = 'checkpoints/autoencoder'\n",
    "    checkpoint_dir_transformer = 'checkpoints/transformer'\n",
    "    results_dir = 'results'\n",
    "    \n",
    "    for dir_path in [checkpoint_dir_ae, checkpoint_dir_transformer, results_dir]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    train_df, val_df, test_df, word_to_idx, idx_to_word, vocab_size, max_length = load_data(csv_path)\n",
    "    \n",
    "    # Update global MAX_LENGTH\n",
    "    global MAX_LENGTH\n",
    "    MAX_LENGTH = max_length\n",
    "    # In your main function\n",
    "    base_path = \"../../\"  # This will go up two directories from your notebook location\n",
    "    \n",
    "    # Create datasets with the corrected base path\n",
    "    train_dataset = ChestXrayDataset(train_df, transform=transform, max_length=MAX_LENGTH, \n",
    "                                    word_to_idx=word_to_idx, base_path=base_path)\n",
    "    val_dataset = ChestXrayDataset(val_df, transform=transform, max_length=MAX_LENGTH, \n",
    "                                  word_to_idx=word_to_idx, base_path=base_path)\n",
    "    test_dataset = ChestXrayDataset(test_df, transform=transform, max_length=MAX_LENGTH, \n",
    "                                   word_to_idx=word_to_idx, base_path=base_path)\n",
    "    \n",
    "    # # Create datasets\n",
    "    # train_dataset = ChestXrayDataset(train_df, transform=transform, max_length=MAX_LENGTH, word_to_idx=word_to_idx)\n",
    "    # val_dataset = ChestXrayDataset(val_df, transform=transform, max_length=MAX_LENGTH, word_to_idx=word_to_idx)\n",
    "    # test_dataset = ChestXrayDataset(test_df, transform=transform, max_length=MAX_LENGTH, word_to_idx=word_to_idx)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Initialize models\n",
    "    encoder = Encoder(LATENT_DIM).to(device)\n",
    "    decoder = Decoder(LATENT_DIM).to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "    \n",
    "    transformer_decoder = DecoderTransformer(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        nhead=NHEAD,\n",
    "        num_layers=NUM_DECODER_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    ).to(device)\n",
    "    \n",
    "    full_model = ImageCaptioningModel(encoder, transformer_decoder).to(device)\n",
    "    \n",
    "    # Define loss and optimizer for transformer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['<pad>'])\n",
    "    optimizer = torch.optim.Adam(full_model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Resume training if specified\n",
    "    if resume_training and resume_from is not None:\n",
    "        print(f\"Resuming training from checkpoint: {resume_from}\")\n",
    "        checkpoint = torch.load(resume_from)\n",
    "        \n",
    "        if 'encoder_state_dict' in checkpoint:\n",
    "            # Resuming autoencoder training\n",
    "            encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "            decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "            discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "            print(f\"Loaded autoencoder checkpoint from epoch {checkpoint['epoch'] + 1}\")\n",
    "        elif 'model_state_dict' in checkpoint:\n",
    "            # Resuming transformer training\n",
    "            full_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(f\"Loaded transformer checkpoint from epoch {checkpoint['epoch'] + 1}\")\n",
    "    \n",
    "    # Step 1: Train autoencoder with GAN\n",
    "    print(\"\\n=== Training Autoencoder-GAN ===\")\n",
    "    encoder, decoder, discriminator = train_autoencoder_gan(\n",
    "        encoder, decoder, discriminator, train_loader, val_loader, device, \n",
    "        num_epochs=NUM_EPOCHS_AE, checkpoint_dir=checkpoint_dir_ae\n",
    "    )\n",
    "    \n",
    "    # Step 2: Train transformer with frozen encoder\n",
    "    print(\"\\n=== Training Transformer with encoded features ===\")\n",
    "    # Load best autoencoder checkpoint\n",
    "    best_ae_checkpoint = torch.load(os.path.join(checkpoint_dir_ae, 'best_model.pth'))\n",
    "    encoder.load_state_dict(best_ae_checkpoint['encoder_state_dict'])\n",
    "    \n",
    "    # Update encoder in full model\n",
    "    full_model.encoder = encoder\n",
    "    \n",
    "    # Freeze encoder weights\n",
    "    for param in full_model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Train transformer\n",
    "    full_model = train_transformer(\n",
    "        full_model, train_loader, val_loader, word_to_idx, idx_to_word,\n",
    "        criterion, optimizer, num_epochs=NUM_EPOCHS_TRANSFORMER, \n",
    "        checkpoint_dir=checkpoint_dir_transformer\n",
    "    )\n",
    "    \n",
    "    # Step 3: Evaluate on test set\n",
    "    print(\"\\n=== Evaluating on Test Set ===\")\n",
    "    # Load best transformer checkpoint\n",
    "    best_transformer_checkpoint = torch.load(os.path.join(checkpoint_dir_transformer, 'best_model.pth'))\n",
    "    full_model.load_state_dict(best_transformer_checkpoint['model_state_dict'])\n",
    "    \n",
    "    metrics = evaluate_model(full_model, test_loader, word_to_idx, idx_to_word, results_dir=results_dir)\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(full_model.state_dict(), os.path.join(results_dir, 'final_model.pth'))\n",
    "    \n",
    "    print(\"\\n=== Training and Evaluation Complete ===\")\n",
    "    print(f\"Final BLEU-1: {metrics['bleu1']:.4f}\")\n",
    "    print(f\"Final BLEU-4: {metrics['bleu4']:.4f}\")\n",
    "    print(f\"Final METEOR: {metrics['meteor']:.4f}\")\n",
    "    print(f\"Final ROUGE-L F1: {metrics['rougeL_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c3e214b-1456-4ca8-8344-96e8317c0e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/joshi.tanm/CSYE-7374/final-project/experiments/stable diffusion\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955d829-d204-4363-a05a-c6a22a0751bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in DataFrame:\n",
      "Unnamed: 0        0\n",
      "final_img_path    0\n",
      "captions          0\n",
      "dtype: int64\n",
      "Max caption length: 125\n",
      "Training samples: 4519, Validation samples: 969, Test samples: 969\n",
      "\n",
      "=== Training Autoencoder-GAN ===\n",
      "Epoch [1/10], Batch [100/142], Recon Loss: 0.5257, Gen Loss: 6.2657, Disc Loss: 0.0027\n",
      "Epoch [1/10], Time: 983.48s, Train Recon Loss: 0.6925, Train Gen Loss: 4.6510, Train Disc Loss: 0.1141, Val Recon Loss: 0.4859\n",
      "Saved best model with validation loss: 0.4859\n",
      "Epoch [2/10], Batch [100/142], Recon Loss: 0.4755, Gen Loss: 7.9654, Disc Loss: 0.0002\n",
      "Epoch [2/10], Time: 678.72s, Train Recon Loss: 0.4632, Train Gen Loss: 7.3685, Train Disc Loss: 0.0006, Val Recon Loss: 0.4485\n",
      "Saved best model with validation loss: 0.4485\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"final_dataset.csv\"  # Replace with your actual CSV path\n",
    "    main(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a02af-38c6-4c66-95bc-214f593fc53a",
   "metadata": {},
   "source": [
    "# Preliminary Results\n",
    "Based on the partial training output visible in the notebook, the autoencoder GAN training was progressing well, with reconstruction loss decreasing from 0.6925 to 0.4485 over just two epochs. This suggests that the model was successfully learning to compress and reconstruct the X-ray images.\n",
    "\n",
    "## Challenges and Future Work\n",
    "This approach is computationally demanding due to:\n",
    "\n",
    "1. The GAN-based autoencoder training\n",
    "2. The subsequent transformer training on the encoded features\n",
    "\n",
    "Future optimizations could include:\n",
    "\n",
    "- Using a more efficient encoder architecture\n",
    "- Implementing progressive training strategies\n",
    "- Exploring different latent space dimensions\n",
    "- Incorporating more medical domain knowledge\n",
    "\n",
    "Despite these challenges, the stable diffusion-inspired approach offers a promising direction for medical image captioning by potentially capturing more nuanced visual features in the latent space compared to standard CNN encoders.\n",
    "\n",
    "# Conclusion\n",
    "This experiment combines ideas from stable diffusion models with transformer-based text generation, creating a powerful pipeline for medical image-to-text report generation. While computationally intensive, the approach has the potential to create better latent representations of medical images, leading to more accurate and detailed report generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
