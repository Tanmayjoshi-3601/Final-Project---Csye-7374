{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275e379d-7b15-40af-b37c-122ac6c19195",
   "metadata": {},
   "source": [
    "# Medical Image to Text Report Generation\n",
    "## Experiment 1: Vision Transformer + Transformer Decoder\n",
    "This notebook implements a deep learning model to generate diagnostic reports from chest X-ray images. The approach uses a Vision Transformer (ViT) for image encoding and a GPT-2 language model for text generation. This is a modern take on medical image captioning that leverages state-of-the-art deep learning architectures.\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e0df5bd-a735-459a-a52b-6b0f6476529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformers import ViTModel, ViTFeatureExtractor, GPT2Config, GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# Download NLTK data for BLEU score calculation\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 768  # Match ViT dimension\n",
    "HIDDEN_DIM = 768\n",
    "NUM_DECODER_LAYERS = 6\n",
    "DROPOUT = 0.1\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 20\n",
    "MAX_LENGTH = 100  # Will be updated based on actual data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae438445-a8ef-4ee6-958b-a96325a02198",
   "metadata": {},
   "source": [
    "## Check GPU Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c3fcc9b-c6f2-496a-96e4-12179f26d0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 15 11:32:04 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              51W / 500W |      8MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0734206a-0e98-41b9-adc2-01620f361187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b962380-2c47-4e83-9f63-cf77365e00e0",
   "metadata": {},
   "source": [
    "## Suppress Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56666637-1da0-4636-a1a8-01850f3aaecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67a621-adaa-4e7f-8d75-06adb6a092f6",
   "metadata": {},
   "source": [
    "## Dataset Class Implementation\n",
    "Custom dataset class for the chest X-ray images and their captions. This handles:\n",
    "\n",
    "- Loading X-ray images from file paths\n",
    "- Path resolution based on the provided base path\n",
    "- Image preprocessing through the ViT feature extractor\n",
    "- Caption tokenization with the GPT-2 tokenizer\n",
    "- Returns properly formatted tensors for both images and captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43929d1b-cc98-4bcb-84b0-9824f769e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, dataframe, feature_extractor=None, tokenizer=None, max_length=100, base_path=None, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.base_path = base_path\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['final_img_path']\n",
    "        caption = self.dataframe.iloc[idx]['captions']\n",
    "        \n",
    "        # Adjust the image path if base_path is provided\n",
    "        if self.base_path:\n",
    "            # Extract the part of the path after 'data/'\n",
    "            if 'data/' in img_path:\n",
    "                relative_path = img_path[img_path.find('data/'):]\n",
    "                img_path = os.path.join(self.base_path, relative_path)\n",
    "            else:\n",
    "                # If 'data/' is not in the path, just join with base_path\n",
    "                img_path = os.path.join(self.base_path, img_path)\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transformations if needed\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Process image with feature extractor if available\n",
    "        if self.feature_extractor:\n",
    "            image_encoding = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "            for k, v in image_encoding.items():\n",
    "                image_encoding[k] = v.squeeze(0)\n",
    "        else:\n",
    "            image_encoding = image\n",
    "            \n",
    "        # Tokenize caption if tokenizer is provided\n",
    "        if self.tokenizer:\n",
    "            caption_encoding = self.tokenizer(\n",
    "                caption, \n",
    "                padding=\"max_length\", \n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            for k, v in caption_encoding.items():\n",
    "                caption_encoding[k] = v.squeeze(0)\n",
    "                \n",
    "            return image_encoding, caption_encoding\n",
    "        \n",
    "        return image_encoding, caption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482a0127-8dd8-4281-93f6-1c28e78eb1f0",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "This is the core model architecture. Key features:\n",
    "\n",
    "- Uses a pretrained Vision Transformer (ViT) as the image encoder\n",
    "- Uses a pretrained GPT-2 language model as the text decoder\n",
    "- Modifies GPT-2 to accept cross-attention from the image encoder\n",
    "- Freezes encoder parameters to leverage transfer learning effectively\n",
    "- Implements both training (forward) and inference (generate_caption) methods\n",
    "- Uses beam search for better caption generation during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed7510b-b532-4bb7-a484-5ba19aad2000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalCaptioningModel(nn.Module):\n",
    "    def __init__(self, vit_model_name=\"google/vit-base-patch16-224\", decoder_model_name=\"gpt2\"):\n",
    "        super(MedicalCaptioningModel, self).__init__()\n",
    "        # Load pretrained ViT encoder\n",
    "        self.encoder = ViTModel.from_pretrained(vit_model_name)\n",
    "        \n",
    "        # Freeze encoder parameters (optional)\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Load GPT-2 decoder configuration and modify for captioning\n",
    "        self.decoder_config = GPT2Config.from_pretrained(decoder_model_name)\n",
    "        self.decoder_config.add_cross_attention = True  # Enable cross-attention\n",
    "        self.decoder_config.is_decoder = True\n",
    "        \n",
    "        # Initialize decoder with modified config\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(\n",
    "            decoder_model_name, \n",
    "            config=self.decoder_config\n",
    "        )\n",
    "        \n",
    "        # Reset some decoder weights for fine-tuning\n",
    "        self.decoder.lm_head.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, pixel_values, input_ids, attention_mask=None, labels=None):\n",
    "        # Encode image\n",
    "        encoder_outputs = self.encoder(pixel_values=pixel_values).last_hidden_state\n",
    "        \n",
    "        # Prepare encoder hidden states for the decoder\n",
    "        encoder_hidden_states = encoder_outputs\n",
    "        \n",
    "        # Decode and generate caption\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            labels=labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        return decoder_outputs\n",
    "    \n",
    "    def generate_caption(self, pixel_values, tokenizer, max_length=50, num_beams=4):\n",
    "        \"\"\"Generate caption for an image using beam search\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Encode image\n",
    "            encoder_outputs = self.encoder(pixel_values=pixel_values).last_hidden_state\n",
    "            \n",
    "            # Generate caption using beam search\n",
    "            generated_ids = self.decoder.generate(\n",
    "                encoder_hidden_states=encoder_outputs,\n",
    "                bos_token_id=tokenizer.bos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                no_repeat_ngram_size=2,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # Decode the generated IDs to text\n",
    "            captions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            return captions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b214d-6433-4d3f-9245-30046873ba1e",
   "metadata": {},
   "source": [
    "## Utility Functions for Training and Evaluation\n",
    "These utility functions handle the various aspects of the machine learning pipeline:\n",
    "\n",
    "- Data loading and preprocessing from CSV file\n",
    "- BLEU score calculation (both per example and corpus-wide)\n",
    "- Model checkpointing and loading\n",
    "- Setting up TensorBoard for logging metrics\n",
    "- Tokenization and detokenization of captions\n",
    "- Plotting learning curves and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc86298-e45a-476d-bf7f-a0f19d3ff5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path):\n",
    "    \"\"\"Load and preprocess data from CSV file.\"\"\"\n",
    "    # Load CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"Missing values in DataFrame:\\n{df.isnull().sum()}\")\n",
    "    \n",
    "    # Determine max caption length for padding\n",
    "    all_captions = df['captions'].tolist()\n",
    "    max_length = max(len(caption.split()) for caption in all_captions) + 2  # +2 for start/end tokens\n",
    "    print(f\"Max caption length: {max_length}\")\n",
    "    \n",
    "    # Split data into train, validation, and test sets (70%, 15%, 15%)\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}, Test samples: {len(test_df)}\")\n",
    "    \n",
    "    return train_df, val_df, test_df, max_length\n",
    "\n",
    "def calculate_bleu_score(references, hypothesis, smooth=True):\n",
    "    \"\"\"Calculate BLEU scores for a single example\"\"\"\n",
    "    smoothing = SmoothingFunction().method1 if smooth else None\n",
    "    \n",
    "    # Calculate BLEU-1 to BLEU-4\n",
    "    bleu1 = sentence_bleu(references, hypothesis, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
    "    bleu2 = sentence_bleu(references, hypothesis, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
    "    bleu3 = sentence_bleu(references, hypothesis, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)\n",
    "    bleu4 = sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "    \n",
    "    return bleu1, bleu2, bleu3, bleu4\n",
    "\n",
    "def calculate_corpus_bleu(list_of_references, hypotheses, smooth=True):\n",
    "    \"\"\"Calculate corpus BLEU scores\"\"\"\n",
    "    bleu1_total, bleu2_total, bleu3_total, bleu4_total = 0, 0, 0, 0\n",
    "    count = 0\n",
    "    \n",
    "    for refs, hyp in zip(list_of_references, hypotheses):\n",
    "        b1, b2, b3, b4 = calculate_bleu_score(refs, hyp, smooth)\n",
    "        bleu1_total += b1\n",
    "        bleu2_total += b2\n",
    "        bleu3_total += b3\n",
    "        bleu4_total += b4\n",
    "        count += 1\n",
    "    \n",
    "    # Average scores\n",
    "    bleu1 = bleu1_total / count if count > 0 else 0\n",
    "    bleu2 = bleu2_total / count if count > 0 else 0\n",
    "    bleu3 = bleu3_total / count if count > 0 else 0\n",
    "    bleu4 = bleu4_total / count if count > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'bleu1': bleu1,\n",
    "        'bleu2': bleu2,\n",
    "        'bleu3': bleu3,\n",
    "        'bleu4': bleu4\n",
    "    }\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, metrics, checkpoint_path):\n",
    "    \"\"\"Save model checkpoint with all training state.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path, device):\n",
    "    \"\"\"Load model checkpoint and return training state.\"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"No checkpoint found at {checkpoint_path}\")\n",
    "        return 0, {}\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint['epoch']+1}\")\n",
    "    return checkpoint['epoch'] + 1, checkpoint.get('metrics', {})\n",
    "\n",
    "def setup_logger(log_dir):\n",
    "    \"\"\"Set up TensorBoard logger.\"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    return SummaryWriter(log_dir)\n",
    "\n",
    "def tokenize_caption(tokenizer, caption):\n",
    "    \"\"\"Convert caption to tokens.\"\"\"\n",
    "    return tokenizer.encode(caption, add_special_tokens=False)\n",
    "\n",
    "def detokenize_caption(tokenizer, tokens):\n",
    "    \"\"\"Convert tokens to caption text.\"\"\"\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "def plot_learning_curves(train_values, val_values, title, ylabel, save_path):\n",
    "    \"\"\"Plot and save learning curves.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_values, label=f'Training {ylabel}')\n",
    "    plt.plot(val_values, label=f'Validation {ylabel}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics(metrics_dict, title, save_path):\n",
    "    \"\"\"Plot and save multiple metrics.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for name, values in metrics_dict.items():\n",
    "        plt.plot(values, label=name.upper())\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4befd0e-1b85-454e-a0cf-395620accbbe",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "This comprehensive training function:\n",
    "\n",
    "- Initializes logging and checkpointing directories\n",
    "- Handles model checkpointing, saving the best model based on validation loss\n",
    "- Implements a full training and validation loop\n",
    "- Calculates and logs training loss, validation loss, and BLEU scores\n",
    "- Displays early training diagnostics to catch issues\n",
    "- Implements learning rate scheduling\n",
    "- Uses gradient clipping to prevent exploding gradients\n",
    "- Visualizes learning curves and BLEU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac15fa2-0f80-4ddd-800a-fe258bd735dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_captioning_model(model, train_loader, val_loader, tokenizer, \n",
    "                          optimizer, criterion, scheduler=None, \n",
    "                          num_epochs=20, \n",
    "                          checkpoint_dir='checkpoints/captioning', \n",
    "                          log_dir='logs/captioning'):\n",
    "    \"\"\"Train the captioning model with efficient checkpointing and logging.\"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(checkpoint_dir, 'samples'), exist_ok=True)\n",
    "    \n",
    "    # Set up logger\n",
    "    writer = setup_logger(log_dir)\n",
    "    \n",
    "    # For tracking metrics\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = {f'bleu{i}': [] for i in range(1, 5)}\n",
    "    \n",
    "    # Try to load checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'latest.pth')\n",
    "    start_epoch, metrics = load_checkpoint(model, optimizer, checkpoint_path, device)\n",
    "    \n",
    "    if metrics:\n",
    "        train_losses = metrics.get('train_losses', [])\n",
    "        val_losses = metrics.get('val_losses', [])\n",
    "        best_val_loss = metrics.get('best_val_loss', float('inf'))\n",
    "        \n",
    "        # Load BLEU scores if available\n",
    "        for i in range(1, 5):\n",
    "            key = f'bleu{i}'\n",
    "            if key in metrics:\n",
    "                bleu_scores[key] = metrics[key]\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Extract batch data \n",
    "            image_encodings, caption_encodings = batch\n",
    "            \n",
    "            # Move to device\n",
    "            pixel_values = image_encodings['pixel_values'].to(device)\n",
    "            input_ids = caption_encodings['input_ids'].to(device)\n",
    "            attention_mask = caption_encodings['attention_mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids  # For calculating loss\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "        # Calculate average training loss\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            writer.add_scalar('captioning/learning_rate', scheduler.get_last_lr()[0], epoch)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_progress_bar = tqdm(val_loader, desc=f\"Validating\")\n",
    "            for batch in val_progress_bar:\n",
    "                # Extract batch data\n",
    "                image_encodings, caption_encodings = batch\n",
    "                \n",
    "                # Move to device\n",
    "                pixel_values = image_encodings['pixel_values'].to(device)\n",
    "                input_ids = caption_encodings['input_ids'].to(device)\n",
    "                attention_mask = caption_encodings['attention_mask'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    pixel_values=pixel_values,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=input_ids\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Generate captions for BLEU score calculation (for a subset)\n",
    "                if len(hypotheses) < 100:  # Limit to 100 examples for speed\n",
    "                    # Generate captions\n",
    "                    batch_size = pixel_values.size(0)\n",
    "                    for i in range(min(batch_size, 5)):  # Process up to 5 per batch\n",
    "                        img = pixel_values[i:i+1]\n",
    "                        \n",
    "                        # Generate caption\n",
    "                        generated_caption = model.generate_caption(img, tokenizer)[0]\n",
    "                        generated_tokens = tokenize_caption(tokenizer, generated_caption)\n",
    "                        \n",
    "                        # Get reference caption\n",
    "                        reference_caption = detokenize_caption(\n",
    "                            tokenizer, \n",
    "                            caption_encodings['input_ids'][i].tolist()\n",
    "                        )\n",
    "                        reference_tokens = [tokenize_caption(tokenizer, reference_caption)]\n",
    "                        \n",
    "                        references.append(reference_tokens)\n",
    "                        hypotheses.append(generated_tokens)\n",
    "                        \n",
    "                        # Save example images with captions (every 5 epochs)\n",
    "                        if epoch % 5 == 0 and len(hypotheses) <= 5:\n",
    "                            # Save visualization code here (omitted for brevity)\n",
    "                            # Will implement in the full version\n",
    "                            pass\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Calculate BLEU scores\n",
    "        bleu_metrics = calculate_corpus_bleu(references, hypotheses, smooth=True)\n",
    "        for key, value in bleu_metrics.items():\n",
    "            bleu_scores[key].append(value)\n",
    "        \n",
    "        # Log metrics\n",
    "        writer.add_scalar('captioning/train_loss_epoch', train_loss, epoch)\n",
    "        writer.add_scalar('captioning/val_loss', val_loss, epoch)\n",
    "        \n",
    "        for key, value in bleu_metrics.items():\n",
    "            writer.add_scalar(f'captioning/{key}', value, epoch)\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        print(f'\\nEpoch [{epoch+1}/{num_epochs}], Time: {time_elapsed:.2f}s, '\n",
    "              f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "              f'BLEU-1: {bleu_metrics[\"bleu1\"]:.4f}, BLEU-4: {bleu_metrics[\"bleu4\"]:.4f}')\n",
    "        \n",
    "        # Early diagnostics\n",
    "        if epoch < 3:\n",
    "            print(\"\\nEarly training diagnostics:\")\n",
    "            for i, (ref, hyp) in enumerate(zip(references[:3], hypotheses[:3])):\n",
    "                print(f\"Generated: {detokenize_caption(tokenizer, hyp)}\")\n",
    "                print(f\"Reference: {detokenize_caption(tokenizer, ref[0])}\")\n",
    "                print(\"---\")\n",
    "        \n",
    "        # Save metrics for checkpoints\n",
    "        metrics = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            **{k: v for k, v in bleu_scores.items()}\n",
    "        }\n",
    "        \n",
    "        # Save latest checkpoint\n",
    "        save_checkpoint(model, optimizer, epoch, metrics, checkpoint_path)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            metrics['best_val_loss'] = best_val_loss\n",
    "            best_checkpoint_path = os.path.join(checkpoint_dir, 'best.pth')\n",
    "            save_checkpoint(model, optimizer, epoch, metrics, best_checkpoint_path)\n",
    "            print(f\"Saved best model with validation loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save periodic checkpoints\n",
    "        if (epoch+1) % 5 == 0 or epoch == num_epochs-1:\n",
    "            epoch_checkpoint_path = os.path.join(checkpoint_dir, f'epoch_{epoch+1}.pth')\n",
    "            save_checkpoint(model, optimizer, epoch, metrics, epoch_checkpoint_path)\n",
    "    \n",
    "    # Plot learning curves at the end of training\n",
    "    plot_learning_curves(\n",
    "        train_losses, val_losses, \n",
    "        'Captioning Model Training and Validation Loss', \n",
    "        'Loss', \n",
    "        os.path.join(checkpoint_dir, 'learning_curve.png')\n",
    "    )\n",
    "    \n",
    "    # Plot BLEU scores\n",
    "    plot_metrics(\n",
    "        bleu_scores,\n",
    "        'BLEU Scores',\n",
    "        os.path.join(checkpoint_dir, 'bleu_scores.png')\n",
    "    )\n",
    "    \n",
    "    writer.close()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b370822-41d3-4fa6-b99d-3bb55b9f46a5",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "The evaluation pipeline includes:\n",
    "\n",
    "- Generating captions for all test images\n",
    "- Calculating BLEU scores without smoothing for a fair assessment\n",
    "- Creating visualizations of model outputs\n",
    "- Saving individual sample images with captions\n",
    "- Creating a grid visualization for easy comparison\n",
    "- Generating an HTML report for interactive viewing\n",
    "- Saving metrics and results to files for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c55917d2-0f1b-4f1b-9355-223bc3cb4248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, tokenizer, results_dir='results'):\n",
    "    \"\"\"Evaluate the model on the test set with visualizations.\"\"\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(results_dir, 'samples'), exist_ok=True)\n",
    "    \n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    # For visualization\n",
    "    results_data = []\n",
    "    visualization_samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc=\"Evaluating\")\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            # Extract batch data\n",
    "            image_encodings, caption_encodings = batch\n",
    "            \n",
    "            # Move to device\n",
    "            pixel_values = image_encodings['pixel_values'].to(device)\n",
    "            input_ids = caption_encodings['input_ids'].to(device)\n",
    "            \n",
    "            # Generate captions for all images in batch\n",
    "            batch_size = pixel_values.size(0)\n",
    "            for j in range(batch_size):\n",
    "                img = pixel_values[j:j+1]\n",
    "                \n",
    "                # Generate caption\n",
    "                generated_caption = model.generate_caption(img, tokenizer)[0]\n",
    "                generated_tokens = tokenize_caption(tokenizer, generated_caption)\n",
    "                \n",
    "                # Get reference caption\n",
    "                reference_caption = detokenize_caption(\n",
    "                    tokenizer, \n",
    "                    caption_encodings['input_ids'][j].tolist()\n",
    "                )\n",
    "                reference_tokens = [tokenize_caption(tokenizer, reference_caption)]\n",
    "                \n",
    "                references.append(reference_tokens)\n",
    "                hypotheses.append(generated_tokens)\n",
    "                \n",
    "                # Store result data\n",
    "                results_data.append({\n",
    "                    'image_idx': i * batch_size + j,\n",
    "                    'generated_caption': generated_caption,\n",
    "                    'reference_caption': reference_caption\n",
    "                })\n",
    "                \n",
    "                # Save sample images with captions (first 20 examples)\n",
    "                if len(visualization_samples) < 20:\n",
    "                    # Convert image tensor to numpy for visualization\n",
    "                    img_np = convert_image_for_display(img.cpu())\n",
    "                    \n",
    "                    # Save individual sample\n",
    "                    plt.figure(figsize=(8, 6))\n",
    "                    plt.imshow(img_np)\n",
    "                    plt.title(f\"Generated: {generated_caption}\\nReference: {reference_caption}\")\n",
    "                    plt.axis('off')\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(os.path.join(results_dir, 'samples', f'sample_{len(visualization_samples)+1}.png'))\n",
    "                    plt.close()\n",
    "                    \n",
    "                    # Store sample for grid visualization\n",
    "                    visualization_samples.append({\n",
    "                        'image': img_np,\n",
    "                        'generated': generated_caption,\n",
    "                        'reference': reference_caption\n",
    "                    })\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    bleu_metrics = calculate_corpus_bleu(references, hypotheses, smooth=False)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    for key, value in bleu_metrics.items():\n",
    "        print(f\"{key.upper()}: {value:.4f}\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    with open(os.path.join(results_dir, 'metrics.json'), 'w') as f:\n",
    "        json.dump(bleu_metrics, f, indent=4)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    results_df.to_csv(os.path.join(results_dir, 'captioning_results.csv'), index=False)\n",
    "    \n",
    "    # Create grid of examples\n",
    "    create_visualization_grid(visualization_samples, results_dir)\n",
    "    \n",
    "    return bleu_metrics\n",
    "\n",
    "def convert_image_for_display(image_tensor):\n",
    "    \"\"\"Convert ViT-processed image tensor to displayable numpy array.\"\"\"\n",
    "    # Check if the image is a single tensor or part of batch\n",
    "    if len(image_tensor.shape) == 4:  # batch of images\n",
    "        image_tensor = image_tensor.squeeze(0)\n",
    "    \n",
    "    # If using the ViT feature extractor, we need to denormalize the image\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    # Denormalize\n",
    "    image_tensor = image_tensor * std + mean\n",
    "    \n",
    "    # Convert to numpy and transpose from (C,H,W) to (H,W,C)\n",
    "    image_np = image_tensor.permute(1, 2, 0).numpy()\n",
    "    \n",
    "    # Ensure values are in valid range [0,1]\n",
    "    image_np = np.clip(image_np, 0, 1)\n",
    "    \n",
    "    return image_np\n",
    "\n",
    "def create_visualization_grid(samples, results_dir, grid_size=(4, 5)):\n",
    "    \"\"\"Create a grid visualization of sample images with captions.\"\"\"\n",
    "    rows, cols = grid_size\n",
    "    fig = plt.figure(figsize=(cols * 4, rows * 4))\n",
    "    \n",
    "    for i, sample in enumerate(samples[:rows*cols]):\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        \n",
    "        # Display image\n",
    "        ax.imshow(sample['image'])\n",
    "        \n",
    "        # Set title with generated and reference captions\n",
    "        gen_caption = textwrap.fill(f\"Gen: {sample['generated']}\", width=40)\n",
    "        ref_caption = textwrap.fill(f\"Ref: {sample['reference']}\", width=40)\n",
    "        ax.set_title(f\"{gen_caption}\\n{ref_caption}\", fontsize=8)\n",
    "        \n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, 'caption_grid.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create HTML report for better viewing of results\n",
    "    create_html_report(samples, results_dir)\n",
    "\n",
    "def create_html_report(samples, results_dir):\n",
    "    \"\"\"Create an HTML report of all samples for easier viewing in a browser.\"\"\"\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Medical Image Captioning Results</title>\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; margin: 20px; }\n",
    "            .sample { margin-bottom: 30px; border: 1px solid #ddd; padding: 15px; border-radius: 5px; }\n",
    "            .sample img { max-width: 100%; max-height: 300px; }\n",
    "            .captions { margin-top: 10px; }\n",
    "            .generated { color: #2c6fbb; }\n",
    "            .reference { color: #2a9d8f; }\n",
    "            h2 { color: #333; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Medical Image Captioning Results</h1>\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        # Save image for the HTML report\n",
    "        img_path = f'samples/sample_{i+1}.png'\n",
    "        \n",
    "        # Add sample to HTML\n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"sample\">\n",
    "            <h2>Sample {i+1}</h2>\n",
    "            <img src=\"{img_path}\" alt=\"Medical Image\">\n",
    "            <div class=\"captions\">\n",
    "                <p class=\"generated\"><strong>Generated:</strong> {sample['generated']}</p>\n",
    "                <p class=\"reference\"><strong>Reference:</strong> {sample['reference']}</p>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write HTML file\n",
    "    with open(os.path.join(results_dir, 'results_report.html'), 'w') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"HTML report created at {os.path.join(results_dir, 'results_report.html')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9770fe4-8d0f-4774-b23f-4071232d48f7",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "This section handles loading and preparing all the data:\n",
    "\n",
    "- Loads dataset from CSV file\n",
    "- Splits into train, validation, and test sets (70/15/15)\n",
    "- Determines the maximum caption length for proper padding\n",
    "- Creates dataset instances for each split\n",
    "- Initializes data loaders with batch processing and parallel workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c1e40d2-e323-42c8-8c0b-e4788abbf555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in DataFrame:\n",
      "Unnamed: 0        0\n",
      "final_img_path    0\n",
      "captions          0\n",
      "dtype: int64\n",
      "Max caption length: 125\n",
      "Training samples: 4519, Validation samples: 969, Test samples: 969\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5dc651c73824174b096f57a1828455a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d36ac76013b4d36b6cd1493fc7f544e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10e1ba6eeac4768b8a7f2a00c42f769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bedfa7429a3c4d5f8bf940c156afc1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5fc615d20a4551bf44929999e564bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f9b78249e04db58635b889193d7d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete. Max sequence length: 125\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "csv_path = \"final_dataset.csv\"\n",
    "base_path = \"../../\"\n",
    "\n",
    "# Load data\n",
    "train_df, val_df, test_df, max_length = load_data(csv_path)\n",
    "MAX_LENGTH = max_length\n",
    "\n",
    "# Load pretrained feature extractor and tokenizer\n",
    "vit_model_name = \"google/vit-base-patch16-224\"\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(vit_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Configure tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.bos_token_id = tokenizer.eos_token_id  # GPT-2 doesn't have a BOS token by default\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ChestXrayDataset(\n",
    "    train_df, \n",
    "    feature_extractor=feature_extractor,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    base_path=base_path\n",
    ")\n",
    "\n",
    "val_dataset = ChestXrayDataset(\n",
    "    val_df, \n",
    "    feature_extractor=feature_extractor,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    base_path=base_path\n",
    ")\n",
    "\n",
    "test_dataset = ChestXrayDataset(\n",
    "    test_df, \n",
    "    feature_extractor=feature_extractor,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    base_path=base_path\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Data preparation complete. Max sequence length: {MAX_LENGTH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350da81b-5fda-4cbc-b339-84bf666c982a",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "- Create the combined ViT + GPT-2 model\n",
    "- Move it to the appropriate device (GPU/CPU)\n",
    "- Set up AdamW optimizer with the learning rate defined earlier\n",
    "- Configure a cosine annealing learning rate scheduler for better convergence\n",
    "- No need for explicit loss function as GPT-2 already calculates it internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7f359e1-ff4a-49a0-8556-f0b41f0b7977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Initializing Pretrained Vision Transformer Captioning Model ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c5e8903fd74c63b8e689eb0281baaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d2f507387c4ddeb8486691526cd5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd49f5606aad469cac6deaedc9cd9b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc3f2fa674b4c7cac06d8439d7ed1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialization complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "print(\"\\n=== Initializing Pretrained Vision Transformer Captioning Model ===\")\n",
    "\n",
    "# Create model\n",
    "captioning_model = MedicalCaptioningModel(\n",
    "    vit_model_name=vit_model_name,\n",
    "    decoder_model_name=\"gpt2\"\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.AdamW(captioning_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "# Loss function is cross entropy (automatically calculated in the model's forward pass)\n",
    "criterion = None  # Already handled in the GPT-2 model\n",
    "\n",
    "print(\"Model initialization complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da245100-0911-4768-b018-6b13bf1fc219",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "This code calls the training function to start the training process:\n",
    "\n",
    "- Passes the model, data loaders, and optimization components\n",
    "- Trains for the specified number of epochs\n",
    "- Handles checkpointing, validation, and metric tracking internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aab3160-f543-455b-8964-45d390768873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Vision Transformer Captioning Model ===\n",
      "No checkpoint found at checkpoints/captioning/latest.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480872fdd85f46ef8765dbba0bf4dbae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1193ab60d9d4596956d091aefdac519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/20], Time: 137.28s, Train Loss: 1.3412, Val Loss: 0.4323, BLEU-1: 0.5466, BLEU-4: 0.4543\n",
      "\n",
      "Early training diagnostics:\n",
      "Generated: ateral chest x-ray showing No acute cardiopulmonaryality.\n",
      "Reference: Lateral chest x-ray showing Negative for acute abnormality.\n",
      "---\n",
      "Generated: ateral chest x-ray showing No acute cardiopulmonary abnorm.\n",
      "Reference: Lateral chest x-ray showing Cardiomegaly and hiatal hernia without an acute abnormality identified.\n",
      "---\n",
      "Generated: ateral chest x-ray showing No acute cardiopulmonary abnorm.\n",
      "Reference: Lateral chest x-ray showing 1. A few basilar XXXX of opacity. This may represent scarring or atelectasis.\n",
      "---\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.4323\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6610a4091d4423b497ab989183e2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d9b447f5314565b83cdd7328f6efed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/20], Time: 134.84s, Train Loss: 0.4124, Val Loss: 0.3371, BLEU-1: 0.5496, BLEU-4: 0.4561\n",
      "\n",
      "Early training diagnostics:\n",
      "Generated: ateral chest x-ray showing No acute cardiopulmonary disease.\n",
      "Reference: Lateral chest x-ray showing Negative for acute abnormality.\n",
      "---\n",
      "Generated: ateral chest x-ray showing No acute cardiopulmonary disease.\n",
      "Reference: Lateral chest x-ray showing Cardiomegaly and hiatal hernia without an acute abnormality identified.\n",
      "---\n",
      "Generated: ateral chest x-ray showing No acute cardiopulmonary disease.\n",
      "Reference: Lateral chest x-ray showing 1. A few basilar XXXX of opacity. This may represent scarring or atelectasis.\n",
      "---\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.3371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366521af92e74308a0874d09da5c42c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24757df913340cc9c27265e71ed84ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/20], Time: 136.98s, Train Loss: 0.3317, Val Loss: 0.2866, BLEU-1: 0.5381, BLEU-4: 0.4517\n",
      "\n",
      "Early training diagnostics:\n",
      "Generated: ateral chest x-ray showing No active disease.\n",
      "Reference: Lateral chest x-ray showing Negative for acute abnormality.\n",
      "---\n",
      "Generated: ateral chest x-ray showing No acute cardiopulmonary findings.\n",
      "Reference: Lateral chest x-ray showing Cardiomegaly and hiatal hernia without an acute abnormality identified.\n",
      "---\n",
      "Generated: ateral chest x-ray showing No acute cardiopulmonary findings.\n",
      "Reference: Lateral chest x-ray showing 1. A few basilar XXXX of opacity. This may represent scarring or atelectasis.\n",
      "---\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.2866\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ee3fca27e14715a5e088334677ba88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2528278cd0e446a98c49f1eba8d964ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/20], Time: 150.19s, Train Loss: 0.2747, Val Loss: 0.2501, BLEU-1: 0.5635, BLEU-4: 0.4779\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.2501\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b9dd882bb5419480ee90e0940319be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefcd856bd084b0b966c5b7a95dd38f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/20], Time: 143.11s, Train Loss: 0.2391, Val Loss: 0.2228, BLEU-1: 0.5549, BLEU-4: 0.4678\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.2228\n",
      "Checkpoint saved to checkpoints/captioning/epoch_5.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d019125ec49f4c748596b000125a077a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f327d5a22d2a4560b57286bb3fca6122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/20], Time: 124.84s, Train Loss: 0.2102, Val Loss: 0.2070, BLEU-1: 0.5377, BLEU-4: 0.4498\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.2070\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4fa3da575f4ab58736567e8f0f751a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da2dd9405034342b48bdaba792a93c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/20], Time: 121.84s, Train Loss: 0.1891, Val Loss: 0.1972, BLEU-1: 0.5370, BLEU-4: 0.4503\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.1972\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b928f2a88b407ba87f92712e13aa12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914450da78444f63a89526c31f54a8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/20], Time: 125.83s, Train Loss: 0.1723, Val Loss: 0.1874, BLEU-1: 0.5560, BLEU-4: 0.4711\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.1874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4973b0124d4517a162696a337b30ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa149bc9047844a2b6e0958ad01d516a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/20], Time: 413.63s, Train Loss: 0.1592, Val Loss: 0.1815, BLEU-1: 0.5465, BLEU-4: 0.4586\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.1815\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be19d684ff8640f5b04eeb289a32ba1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770bde044c5c4e59ba94c5638b2c3fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/20], Time: 123.44s, Train Loss: 0.1477, Val Loss: 0.1763, BLEU-1: 0.5354, BLEU-4: 0.4516\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.1763\n",
      "Checkpoint saved to checkpoints/captioning/epoch_10.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1ecb3689164f7ea1c01505c38ab285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1c5cf7971c481b8d802acaca83a02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [11/20], Time: 118.27s, Train Loss: 0.1370, Val Loss: 0.1701, BLEU-1: 0.5416, BLEU-4: 0.4558\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.1701\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b82edc961124c288212424f22a0248c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca859ef24ba40e7825a538b6abbd124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [12/20], Time: 131.03s, Train Loss: 0.1278, Val Loss: 0.1673, BLEU-1: 0.5432, BLEU-4: 0.4599\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.1673\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76f07b2813d4502b89a5c5f50a164e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba79acd6adc40bfb84edcdd3bbd0e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [13/20], Time: 132.22s, Train Loss: 0.1186, Val Loss: 0.1621, BLEU-1: 0.5586, BLEU-4: 0.4738\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.1621\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f876b5dbcc4bcc89e48a146a8b9c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199d3cef865c4362b50dee950dce9463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [14/20], Time: 130.61s, Train Loss: 0.1118, Val Loss: 0.1625, BLEU-1: 0.5447, BLEU-4: 0.4608\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc884a333a5d47e4836a66a2e44426c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0ab949eb184b2f95bb996b60c550c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [15/20], Time: 122.35s, Train Loss: 0.1063, Val Loss: 0.1592, BLEU-1: 0.5422, BLEU-4: 0.4556\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.1592\n",
      "Checkpoint saved to checkpoints/captioning/epoch_15.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fbf12ae70f46dfba1629f9c0647118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260c929578e34a8e98930e1db7d2548a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [16/20], Time: 125.71s, Train Loss: 0.1015, Val Loss: 0.1581, BLEU-1: 0.5526, BLEU-4: 0.4728\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.1581\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a23de17fc7489f98b46a056a112f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d964f537802145fc9ef42dd488f72649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [17/20], Time: 132.10s, Train Loss: 0.0981, Val Loss: 0.1571, BLEU-1: 0.5448, BLEU-4: 0.4565\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.1571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46eff1a6756b40f193db73f2068165b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a59e9d692c7433291b8650368334924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [18/20], Time: 123.73s, Train Loss: 0.0962, Val Loss: 0.1571, BLEU-1: 0.5422, BLEU-4: 0.4560\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.1571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9eee237ee3647e6a76190a09d9392fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffae6914339d483d825486ccb05ad9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [19/20], Time: 126.92s, Train Loss: 0.0948, Val Loss: 0.1568, BLEU-1: 0.5406, BLEU-4: 0.4522\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.1568\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfae473ff0114d8a908dd0f3cffaa4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/20:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c08e3bf1d5463a8911c15511d39bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [20/20], Time: 128.45s, Train Loss: 0.0937, Val Loss: 0.1567, BLEU-1: 0.5361, BLEU-4: 0.4481\n",
      "Checkpoint saved to checkpoints/captioning/latest.pth\n",
      "Checkpoint saved to checkpoints/captioning/best.pth\n",
      "Saved best model with validation loss: 0.1567\n",
      "Checkpoint saved to checkpoints/captioning/epoch_20.pth\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== Training Vision Transformer Captioning Model ===\")\n",
    "\n",
    "# Train the model\n",
    "captioning_model = train_captioning_model(\n",
    "    captioning_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    tokenizer,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    scheduler,\n",
    "    num_epochs=NUM_EPOCHS\n",
    ")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2453c863-f9c1-42df-a2bf-3054a5990782",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "The final step is evaluating the model on the test set:\n",
    "\n",
    "- Loads the best model weights based on validation performance\n",
    "- Runs the evaluation function on the test dataset\n",
    "- Reports final BLEU scores and saves model weights\n",
    "- Creates visualizations and a detailed HTML report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10eedd0d-bb4f-44a8-a2f3-57359eedee55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating on Test Set ===\n",
      "Loaded checkpoint from epoch 20\n",
      "Loaded best model for evaluation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a6f5f2cc374dcf94206551dfbfc4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "BLEU1: 0.5293\n",
      "BLEU2: 0.4905\n",
      "BLEU3: 0.4659\n",
      "BLEU4: 0.4408\n",
      "HTML report created at results/results_report.html\n",
      "\n",
      "=== Training and Evaluation Complete ===\n",
      "Final BLEU-1: 0.5293\n",
      "Final BLEU-4: 0.4408\n"
     ]
    }
   ],
   "source": [
    "#  Evaluate on test set\n",
    "print(\"\\n=== Evaluating on Test Set ===\")\n",
    "\n",
    "# Load best model\n",
    "best_model_path = 'checkpoints/captioning/best.pth'\n",
    "if os.path.exists(best_model_path):\n",
    "    _, _ = load_checkpoint(captioning_model, None, best_model_path, device)\n",
    "    print(\"Loaded best model for evaluation\")\n",
    "\n",
    "# Evaluate\n",
    "metrics = evaluate_model(captioning_model, test_loader, tokenizer)\n",
    "\n",
    "print(\"\\n=== Training and Evaluation Complete ===\")\n",
    "print(f\"Final BLEU-1: {metrics['bleu1']:.4f}\")\n",
    "print(f\"Final BLEU-4: {metrics['bleu4']:.4f}\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(captioning_model.state_dict(), 'results/final_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20486eed-f80e-4a2f-ad26-e2d7227cb88a",
   "metadata": {},
   "source": [
    "# Results and Observations\n",
    "In this experiment, the Vision Transformer + GPT-2 architecture for medical image captioning achieved promising results:\n",
    "\n",
    "- Final BLEU-1 score: 0.5293\n",
    "- Final BLEU-4 score: 0.4408\n",
    "\n",
    "These scores indicate good alignment between generated captions and ground truth references. The training curve showed stable convergence, with validation loss decreasing consistently over epochs.\n",
    "\n",
    "The model successfully learned to identify important radiological findings in chest X-rays and generate appropriate descriptive text. While there's always room for improvement, this experiment demonstrates the viability of using transformer-based architectures for medical image captioning tasks.\n",
    "\n",
    "Future work might include:\n",
    "1. Further fine-tuning hyperparameters\n",
    "2. Experimenting with different pretrained backbones\n",
    "3. Implementing attention visualization to enhance interpretability\n",
    "4. Incorporating domain-specific medical knowledge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bfb4d9-2e9f-4860-8348-76f9eb420e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
